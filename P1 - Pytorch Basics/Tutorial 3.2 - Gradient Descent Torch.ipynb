{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(1,10)\n",
    "y = X*2\n",
    "# y = w*x - To predict w as 2\n",
    "w = torch.tensor(0.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return(w*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_pred):\n",
    "    return(((y - y_pred)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Loss} = MSE = \\frac{1}{N}(y_{pred} - y)^2 = \\frac{1}{N}(wx-y)^2$$\n",
    "$$\\frac{d \\text{(Loss)}}{dw} = \\frac{2x}{N}(wx-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient(x, y, y_pred):\n",
    "#     return(np.dot(x,(y_pred-y)).mean()) # Not required as there is a backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5): 0.000\n",
      "Actual f(5) = 10\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction before training: f(5): {forward(5):.3f}')\n",
    "print(f'Actual f(5) = {y[torch.where(X==5)].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =0.005\n",
    "n_iters = 100\n",
    "total_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; Weight: 0.633; Loss: 126.6667\n",
      "Epoch 2; Weight: 1.066; Loss: 59.1463\n",
      "Epoch 3; Weight: 1.362; Loss: 27.6180\n",
      "Epoch 4; Weight: 1.564; Loss: 12.8961\n",
      "Epoch 5; Weight: 1.702; Loss: 6.0218\n",
      "Epoch 6; Weight: 1.796; Loss: 2.8118\n",
      "Epoch 7; Weight: 1.861; Loss: 1.3130\n",
      "Epoch 8; Weight: 1.905; Loss: 0.6131\n",
      "Epoch 9; Weight: 1.935; Loss: 0.2863\n",
      "Epoch 10; Weight: 1.956; Loss: 0.1337\n",
      "Epoch 11; Weight: 1.970; Loss: 0.0624\n",
      "Epoch 12; Weight: 1.979; Loss: 0.0291\n",
      "Epoch 13; Weight: 1.986; Loss: 0.0136\n",
      "Epoch 14; Weight: 1.990; Loss: 0.0064\n",
      "Epoch 15; Weight: 1.993; Loss: 0.0030\n",
      "Epoch 16; Weight: 1.995; Loss: 0.0014\n",
      "Epoch 17; Weight: 1.997; Loss: 0.0006\n",
      "Epoch 18; Weight: 1.998; Loss: 0.0003\n",
      "Epoch 19; Weight: 1.999; Loss: 0.0001\n",
      "Epoch 20; Weight: 1.999; Loss: 0.0001\n",
      "Epoch 21; Weight: 1.999; Loss: 0.0000\n",
      "Epoch 22; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 23; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 24; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 25; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 26; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 27; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 28; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 29; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 30; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 31; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 32; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 33; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 34; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 35; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 36; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 37; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 38; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 39; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 40; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 41; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 42; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 43; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 44; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 45; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 46; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 47; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 48; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 49; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 50; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 51; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 52; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 53; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 54; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 55; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 56; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 57; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 58; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 59; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 60; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 61; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 62; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 63; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 64; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 65; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 66; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 67; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 68; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 69; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 70; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 71; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 72; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 73; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 74; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 75; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 76; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 77; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 78; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 79; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 80; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 81; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 82; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 83; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 84; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 85; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 86; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 87; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 88; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 89; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 90; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 91; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 92; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 93; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 94; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 95; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 96; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 97; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 98; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 99; Weight: 2.000; Loss: 0.0000\n",
      "Epoch 100; Weight: 2.000; Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "    loss_func = loss(y, y_pred)\n",
    "    total_loss.append(loss_func)\n",
    "\n",
    "    #Derivative of loss\n",
    "    (loss_func.sum()).backward() # d(Loss)/dw = w.grad\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "    #zero the gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    #update the weight\n",
    "    \n",
    "\n",
    "    print(f'Epoch {epoch+1}; Weight: {w:.3f}; Loss: {loss_func:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: f(5): 10.000\n",
      "Actual f(5) = 10\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction after training: f(5): {forward(5):.3f}')\n",
    "print(f'Actual f(5) = {y[torch.where(X==5)].item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
