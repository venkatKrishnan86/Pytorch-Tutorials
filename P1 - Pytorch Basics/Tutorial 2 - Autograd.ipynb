{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "For calculation of gradients\n",
    "\n",
    "Used with `autograd` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2254, 0.4946, 0.0936], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3 ,requires_grad=True) # Need to specify requires_grad = True for calculation of gradients, by default it is False\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph\n",
    "\n",
    "Any operation on this variable x which has `required_grad = True`, will be done using a computational graph. For example:\n",
    "\n",
    "$$y = x + 2$$\n",
    "\n",
    "will result in a graph as shown below -\n",
    "<p align=\"center\">\n",
    "<img src=\"../images/Computational_Graph.png\" style=\"width:600px;height:300px;\">\n",
    "</p>\n",
    "\n",
    "Using the back propagation method, the gradients can be computed and the `grad_fn` attribute varies based on the operation -\n",
    "- `AddBackward0`: Back Propagation for addition operations\n",
    "- `MulBackward0`: Back Propagation for multiplication operations\n",
    "- `MeanBackward0`: Back Propagation for mean calculations\n",
    "and so on...\n",
    "\n",
    "This `grad_fn` attribute tracks the history of computations done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4507, 2.9892, 2.1873], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = (2*x)+2 # Will create a computational graph\n",
    "print(y) # A grad_fn attribute is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.0119, 17.8701,  9.5685], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 2*(y**2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.1502, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z1 = z.mean()\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()` function on a variable creates the gradient of that variable with respect to the variable assigned `requires_grad = True`. It uses calculus to calculate the derivative (`grad_fn` decides how the derivative will be computed) and **not** numerical analysis approximation.\n",
    "\n",
    "It strictly needs the tensor to have a **single** value, if not tensor passed inside backward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1.backward() # dz1/dx (backward() requires it to be a single value\n",
    "# Will work only if requires_grad = True, otherwise it will create a Runtime Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x.grad` is created which is basically $\\frac{dz1}{dx}$\n",
    "\n",
    "This would be -\n",
    "$$y = 2x+2$$\n",
    "$$z = 2(y)^2 = 8(x+1)^2$$\n",
    "$$z1 = z/3$$\n",
    "$$\\frac{dz1}{dx} = \\frac{dz1}{dz} \\times \\frac{dz}{dx} = \\frac{16}{3}(x+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.5352, 7.9711, 5.8328])\n",
      "tensor([6.5352, 7.9711, 5.8328], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(16*(x+1)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the background -\n",
    "\n",
    "It creates the Vector - Jacobian product which is shown below\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/Jacobian_Vector_Product.png\" style=\"width:600px;height:200px;\">\n",
    "</p>\n",
    "\n",
    "where \n",
    "- $J$: Jacobian Matrix\n",
    "- $v$: Gradient Vector\n",
    "- The product results in the gradients we need for Back Propagation\n",
    "\n",
    "In short this calculation is the **Chain rule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want dz/dx\n",
    "x1 = torch.rand(3 ,requires_grad=True)\n",
    "y1 = 2*x1 + 2\n",
    "z1 = 2*(y1**2)\n",
    "z2 = z1.sum()\n",
    "z2.backward() # This will calculate dz/dx as dz2/dz = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here -\n",
    "$$\\frac{dz_2}{dx_1} = 16(x+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.0744, 27.6490, 27.1790])\n",
      "tensor([20.0744, 27.6490, 27.1790], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad)\n",
    "print(16*(x1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29.2486, 17.2806, 25.7376])\n",
      "tensor([29.2486, 17.2806, 25.7376], grad_fn=<MulBackward0>)\n",
      "tensor([29.2486, 17.2806, 25.7376], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Or we pass a vector of same size in the backward() function\n",
    "x2 = torch.rand(3 ,requires_grad=True)\n",
    "y2 = 2*x2 + 2\n",
    "z2 = 2*(y2**2)\n",
    "v2 = torch.ones_like(z2) \n",
    "z2.backward(v2) # Results in (dz2/dx2)/v2, so we will need to multiply v2 later\n",
    "print(x2.grad)\n",
    "print(16*(x2+1))\n",
    "print(16*(x2+1)*v2) # Not needed as v2 is just a vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5111, 40.0987, 13.2673])\n",
      "tensor([20.7550, 41.3589, 21.6156], grad_fn=<MulBackward0>)\n",
      "tensor([ 1.5111, 40.0987, 13.2673], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Lets try v2 to be something else\n",
    "x2 = torch.randn(3 ,requires_grad=True)\n",
    "y2 = 2*x2 + 2\n",
    "z2 = 2*(y2**2)\n",
    "v2 = torch.rand(len(z2))\n",
    "z2.backward(v2)\n",
    "print(x2.grad)\n",
    "print(16*(x2+1)) # Won't work\n",
    "print(16*(x2+1)*v2) # Will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making `requires_grad = False` later in the calculations\n",
    "\n",
    "Once back propagation is over, it is best to deactivate requires_grad while updating the parameters in Neural Networks\n",
    "\n",
    "Can be done in three ways -\n",
    "- **`x.requires_grad_(False)`**: Setting x to have `requires_grad = False`\n",
    "- **`x.detach()`**: Creates a duplicate of the tensor with `requires_grad = False`\n",
    "- **`with torch.no_grad():`**: Writing inside this block ensures `required_grad = False` inside it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `requires_grad_(False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2254, 0.4946, 0.0936], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2254, 0.4946, 0.0936])\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(False) # the underscore in the end means that requires_grad in x has been set to False inplace\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4489,  1.9851,  0.0558], requires_grad=True)\n",
      "tensor([-1.4489,  1.9851,  0.0558])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "x.detach_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1504,  1.2185, -0.2486], requires_grad=True)\n",
      "x: tensor([ 0.1504,  1.2185, -0.2486], requires_grad=True)\n",
      "y: tensor([ 0.1504,  1.2185, -0.2486])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print('x:',x) # x is not affected here as it didn't call the detach() function with an underscore (not inplace)\n",
    "print('y:',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9801, -0.3551,  1.2238], requires_grad=True)\n",
      "y: tensor([1.0199, 1.6449, 3.2238], grad_fn=<AddBackward0>)\n",
      "y: tensor([1.0199, 1.6449, 3.2238])\n",
      "tensor([-0.9801, -0.3551,  1.2238], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print('y:',y) # Here y has the grad_fn\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print('y:',y) # Here y does not have grad_fn\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "tensor([6., 6., 6.])\n",
      "tensor([9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Lets take an example\n",
    "weights = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    output = (weights*3).sum() # y = weights*3, output = y.sum()\n",
    "    output.backward() # Should give d(output)/d(y) * d(y)/d(weights) = 1 * [3,3,3] = [3,3,3]\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason this gets added is because grad needs to be reset after it has run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "tensor([3., 3., 3.])\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(3, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    output = (weights*3).sum() # y = weights*3, output = y.sum()\n",
    "    output.backward() # Should give d(output)/d(y) * d(y)/d(weights) = 1 * [3,3,3] = [3,3,3]\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing it using the `optim` class in torch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(3, requires_grad=True)\n",
    "# optimizer = torch.optim.SGD(weights, lr=0.01) #SGD: Stochastic Gradient Descent\n",
    "# optimizer.step() # updates parameters\n",
    "# optimizer.zero_grad() # Same as .grad.zero_() for the parameters in optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using `backward()` function -----> call `.grad.zero_()` ------> Make `requires_grad = False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pic credits - The Python Engineer YT channel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193f6b5c64d175a70f8bc370a8e28557b54eddf9787b8dde324aa4d68183bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
