{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # For Apple M1 Chip\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # For cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 100\n",
    "num_classes = 10 # 0 to 9 digits\n",
    "num_epochs = 50\n",
    "batch_size = 128 # Preferred to be a power of 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = '../datasets', \n",
    "    train = True, \n",
    "    transform = transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = '../datasets', \n",
    "    train = False, \n",
    "    transform = transforms.ToTensor(), \n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]) tensor([1, 9, 1, 5, 2, 2, 5, 0, 8, 0, 3, 7, 5, 8, 1, 1, 7, 5, 8, 3, 3, 1, 9, 2,\n",
      "        4, 5, 3, 3, 4, 2, 6, 0, 7, 9, 7, 3, 8, 3, 4, 9, 4, 5, 4, 5, 1, 0, 4, 5,\n",
      "        5, 0, 1, 6, 5, 7, 0, 1, 8, 6, 3, 6, 9, 9, 1, 6, 5, 2, 5, 1, 8, 4, 5, 6,\n",
      "        4, 4, 3, 1, 7, 1, 9, 6, 1, 9, 9, 2, 1, 7, 9, 7, 9, 2, 1, 7, 2, 9, 2, 6,\n",
      "        1, 2, 7, 8, 9, 1, 4, 8, 3, 5, 4, 8, 5, 6, 2, 7, 9, 2, 2, 8, 9, 3, 8, 1,\n",
      "        6, 5, 0, 3, 2, 6, 7, 5])\n",
      "Input and Labels shape: torch.Size([128, 1, 28, 28]) torch.Size([128])\n",
      "Unique Labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "examples = next(iter(train_loader))\n",
    "inputs, labels = examples\n",
    "print(inputs, labels)\n",
    "print('Input and Labels shape:', inputs.shape, labels.shape) # 28, 28 is the image size, 1 is for one colour channel (Not RGB but grayscale)\n",
    "print('Unique Labels:', labels.unique())                     # 128 is the label & batch size as each label is assigned a different value from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For displaying the dataset MNIST\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(inputs[i][0]) # 6 from batch-size of 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        # No softmax function here as it is already applied in the CrossEntropyLoss class in torch.nn\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden = [], num_classes=1, activations = [], seed=0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_stack = []\n",
    "        if(len(hidden) != len(activations)-1):\n",
    "            raise IndexError(\"Lengths passed for hidden and activations are not right\")\n",
    "        if(len(hidden) == 0):\n",
    "            self.linear_stack.append(nn.Linear(input_size,num_classes))\n",
    "            if(activations[0] == 'sigmoid'):\n",
    "                self.linear_stack.append(nn.Sigmoid())\n",
    "            elif(activations[0] == 'relu'):\n",
    "                self.linear_stack.append(nn.ReLU())\n",
    "            elif(activations[0] == 'tanh'):\n",
    "                self.linear_stack.append(nn.Tanh())\n",
    "            elif(activations[0] == 'softplus'):\n",
    "                self.linear_stack.append(nn.Softplus())\n",
    "        else:\n",
    "            self.linear_stack.append(nn.Linear(input_size,hidden[0]))\n",
    "            if(activations[0] == 'sigmoid'):\n",
    "                self.linear_stack.append(nn.Sigmoid())\n",
    "            elif(activations[0] == 'relu'):\n",
    "                self.linear_stack.append(nn.ReLU())\n",
    "            elif(activations[0] == 'tanh'):\n",
    "                self.linear_stack.append(nn.Tanh())\n",
    "            elif(activations[0] == 'softplus'):\n",
    "                self.linear_stack.append(nn.Softplus())\n",
    "\n",
    "            for i in range(len(hidden)-1):\n",
    "                self.linear_stack.append(nn.Linear(hidden[i],hidden[i+1]))\n",
    "                if(activations[i+1] == 'sigmoid'):\n",
    "                    self.linear_stack.append(nn.Sigmoid())\n",
    "                elif(activations[i+1] == 'relu'):\n",
    "                    self.linear_stack.append(nn.ReLU())\n",
    "                elif(activations[i+1] == 'tanh'):\n",
    "                    self.linear_stack.append(nn.Tanh())\n",
    "                elif(activations[i+1] == 'softplus'):\n",
    "                    self.linear_stack.append(nn.Softplus())\n",
    "            \n",
    "            self.linear_stack.append(nn.Linear(hidden[-1],num_classes))\n",
    "            if(activations[-1] == 'sigmoid'):\n",
    "                self.linear_stack.append(nn.Sigmoid())\n",
    "            elif(activations[-1] == 'relu'):\n",
    "                self.linear_stack.append(nn.ReLU())\n",
    "            elif(activations[-1] == 'tanh'):\n",
    "                self.linear_stack.append(nn.Tanh())\n",
    "            elif(activations[-1] == 'softplus'):\n",
    "                self.linear_stack.append(nn.Softplus())\n",
    "        self.neural_net = nn.Sequential(*self.linear_stack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.neural_net(x)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "model2 = NeuralNet2(input_size = input_size, hidden = [100, 80, 50], num_classes = num_classes, activations = ['relu', 'tanh', 'tanh', None]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_hat, y_true):\n",
    "    return torch.sum((y_hat - F.one_hot(y_true))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-5)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107d3a370>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 100]          78,500\n",
      "              ReLU-2                  [-1, 100]               0\n",
      "            Linear-3                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(copy.deepcopy(model1).to('cpu'), (28*28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 100]          78,500\n",
      "              ReLU-3                  [-1, 100]               0\n",
      "            Linear-4                   [-1, 80]           8,080\n",
      "              Tanh-5                   [-1, 80]               0\n",
      "            Linear-6                   [-1, 50]           4,050\n",
      "              Tanh-7                   [-1, 50]               0\n",
      "            Linear-8                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 91,140\n",
      "Trainable params: 91,140\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 0.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(copy.deepcopy(model2).to('cpu'), (28*28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.compile(model1) # For pytorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50; Step 100/469; Loss = 116.2877\n",
      "Epoch 1/50; Step 200/469; Loss = 105.2676\n",
      "Epoch 1/50; Step 300/469; Loss = 93.6292\n",
      "Epoch 1/50; Step 400/469; Loss = 92.6937\n",
      "Epoch 2/50; Step 100/469; Loss = 80.0421\n",
      "Epoch 2/50; Step 200/469; Loss = 74.6844\n",
      "Epoch 2/50; Step 300/469; Loss = 72.9348\n",
      "Epoch 2/50; Step 400/469; Loss = 67.6321\n",
      "Epoch 3/50; Step 100/469; Loss = 64.7414\n",
      "Epoch 3/50; Step 200/469; Loss = 61.5698\n",
      "Epoch 3/50; Step 300/469; Loss = 57.0436\n",
      "Epoch 3/50; Step 400/469; Loss = 58.1653\n",
      "Epoch 4/50; Step 100/469; Loss = 58.4062\n",
      "Epoch 4/50; Step 200/469; Loss = 49.5815\n",
      "Epoch 4/50; Step 300/469; Loss = 56.1636\n",
      "Epoch 4/50; Step 400/469; Loss = 54.1621\n",
      "Epoch 5/50; Step 100/469; Loss = 49.0137\n",
      "Epoch 5/50; Step 200/469; Loss = 46.1711\n",
      "Epoch 5/50; Step 300/469; Loss = 48.5054\n",
      "Epoch 5/50; Step 400/469; Loss = 46.0907\n",
      "Epoch 6/50; Step 100/469; Loss = 51.9772\n",
      "Epoch 6/50; Step 200/469; Loss = 53.3300\n",
      "Epoch 6/50; Step 300/469; Loss = 48.9134\n",
      "Epoch 6/50; Step 400/469; Loss = 50.8237\n",
      "Epoch 7/50; Step 100/469; Loss = 45.0381\n",
      "Epoch 7/50; Step 200/469; Loss = 44.8746\n",
      "Epoch 7/50; Step 300/469; Loss = 43.4540\n",
      "Epoch 7/50; Step 400/469; Loss = 43.2647\n",
      "Epoch 8/50; Step 100/469; Loss = 44.3222\n",
      "Epoch 8/50; Step 200/469; Loss = 45.9280\n",
      "Epoch 8/50; Step 300/469; Loss = 40.0424\n",
      "Epoch 8/50; Step 400/469; Loss = 44.2375\n",
      "Epoch 9/50; Step 100/469; Loss = 40.4873\n",
      "Epoch 9/50; Step 200/469; Loss = 41.6611\n",
      "Epoch 9/50; Step 300/469; Loss = 43.9378\n",
      "Epoch 9/50; Step 400/469; Loss = 36.5114\n",
      "Epoch 10/50; Step 100/469; Loss = 41.8034\n",
      "Epoch 10/50; Step 200/469; Loss = 36.5679\n",
      "Epoch 10/50; Step 300/469; Loss = 36.7801\n",
      "Epoch 10/50; Step 400/469; Loss = 39.2114\n",
      "Epoch 11/50; Step 100/469; Loss = 32.3079\n",
      "Epoch 11/50; Step 200/469; Loss = 37.6611\n",
      "Epoch 11/50; Step 300/469; Loss = 39.4723\n",
      "Epoch 11/50; Step 400/469; Loss = 42.7479\n",
      "Epoch 12/50; Step 100/469; Loss = 36.8663\n",
      "Epoch 12/50; Step 200/469; Loss = 33.6049\n",
      "Epoch 12/50; Step 300/469; Loss = 36.5083\n",
      "Epoch 12/50; Step 400/469; Loss = 31.4272\n",
      "Epoch 13/50; Step 100/469; Loss = 35.2873\n",
      "Epoch 13/50; Step 200/469; Loss = 36.5921\n",
      "Epoch 13/50; Step 300/469; Loss = 36.3162\n",
      "Epoch 13/50; Step 400/469; Loss = 36.5100\n",
      "Epoch 14/50; Step 100/469; Loss = 33.6176\n",
      "Epoch 14/50; Step 200/469; Loss = 32.7519\n",
      "Epoch 14/50; Step 300/469; Loss = 30.1888\n",
      "Epoch 14/50; Step 400/469; Loss = 31.2949\n",
      "Epoch 15/50; Step 100/469; Loss = 33.6410\n",
      "Epoch 15/50; Step 200/469; Loss = 31.6457\n",
      "Epoch 15/50; Step 300/469; Loss = 34.9628\n",
      "Epoch 15/50; Step 400/469; Loss = 38.0232\n",
      "Epoch 16/50; Step 100/469; Loss = 31.6284\n",
      "Epoch 16/50; Step 200/469; Loss = 31.9307\n",
      "Epoch 16/50; Step 300/469; Loss = 30.7077\n",
      "Epoch 16/50; Step 400/469; Loss = 28.9707\n",
      "Epoch 17/50; Step 100/469; Loss = 28.7438\n",
      "Epoch 17/50; Step 200/469; Loss = 27.1099\n",
      "Epoch 17/50; Step 300/469; Loss = 23.9863\n",
      "Epoch 17/50; Step 400/469; Loss = 30.2742\n",
      "Epoch 18/50; Step 100/469; Loss = 34.1702\n",
      "Epoch 18/50; Step 200/469; Loss = 32.6135\n",
      "Epoch 18/50; Step 300/469; Loss = 30.9223\n",
      "Epoch 18/50; Step 400/469; Loss = 34.1711\n",
      "Epoch 19/50; Step 100/469; Loss = 28.5037\n",
      "Epoch 19/50; Step 200/469; Loss = 27.9995\n",
      "Epoch 19/50; Step 300/469; Loss = 31.6225\n",
      "Epoch 19/50; Step 400/469; Loss = 28.7256\n",
      "Epoch 20/50; Step 100/469; Loss = 33.7396\n",
      "Epoch 20/50; Step 200/469; Loss = 32.3839\n",
      "Epoch 20/50; Step 300/469; Loss = 28.0334\n",
      "Epoch 20/50; Step 400/469; Loss = 24.4539\n",
      "Epoch 21/50; Step 100/469; Loss = 29.9279\n",
      "Epoch 21/50; Step 200/469; Loss = 25.8593\n",
      "Epoch 21/50; Step 300/469; Loss = 33.1419\n",
      "Epoch 21/50; Step 400/469; Loss = 29.5974\n",
      "Epoch 22/50; Step 100/469; Loss = 23.5689\n",
      "Epoch 22/50; Step 200/469; Loss = 26.9072\n",
      "Epoch 22/50; Step 300/469; Loss = 25.6724\n",
      "Epoch 22/50; Step 400/469; Loss = 30.1080\n",
      "Epoch 23/50; Step 100/469; Loss = 25.8251\n",
      "Epoch 23/50; Step 200/469; Loss = 27.2292\n",
      "Epoch 23/50; Step 300/469; Loss = 27.2470\n",
      "Epoch 23/50; Step 400/469; Loss = 21.7498\n",
      "Epoch 24/50; Step 100/469; Loss = 30.3136\n",
      "Epoch 24/50; Step 200/469; Loss = 31.8061\n",
      "Epoch 24/50; Step 300/469; Loss = 26.0372\n",
      "Epoch 24/50; Step 400/469; Loss = 22.7072\n",
      "Epoch 25/50; Step 100/469; Loss = 28.8923\n",
      "Epoch 25/50; Step 200/469; Loss = 30.9271\n",
      "Epoch 25/50; Step 300/469; Loss = 28.3206\n",
      "Epoch 25/50; Step 400/469; Loss = 25.4340\n",
      "Epoch 26/50; Step 100/469; Loss = 24.4570\n",
      "Epoch 26/50; Step 200/469; Loss = 23.4927\n",
      "Epoch 26/50; Step 300/469; Loss = 24.1474\n",
      "Epoch 26/50; Step 400/469; Loss = 23.4129\n",
      "Epoch 27/50; Step 100/469; Loss = 21.3988\n",
      "Epoch 27/50; Step 200/469; Loss = 28.3499\n",
      "Epoch 27/50; Step 300/469; Loss = 25.4220\n",
      "Epoch 27/50; Step 400/469; Loss = 23.5547\n",
      "Epoch 28/50; Step 100/469; Loss = 28.8835\n",
      "Epoch 28/50; Step 200/469; Loss = 21.4051\n",
      "Epoch 28/50; Step 300/469; Loss = 26.1049\n",
      "Epoch 28/50; Step 400/469; Loss = 26.6341\n",
      "Epoch 29/50; Step 100/469; Loss = 23.2346\n",
      "Epoch 29/50; Step 200/469; Loss = 26.2058\n",
      "Epoch 29/50; Step 300/469; Loss = 23.5176\n",
      "Epoch 29/50; Step 400/469; Loss = 26.2097\n",
      "Epoch 30/50; Step 100/469; Loss = 22.6709\n",
      "Epoch 30/50; Step 200/469; Loss = 29.5877\n",
      "Epoch 30/50; Step 300/469; Loss = 21.9708\n",
      "Epoch 30/50; Step 400/469; Loss = 28.9933\n",
      "Epoch 31/50; Step 100/469; Loss = 24.8063\n",
      "Epoch 31/50; Step 200/469; Loss = 25.3845\n",
      "Epoch 31/50; Step 300/469; Loss = 22.8869\n",
      "Epoch 31/50; Step 400/469; Loss = 23.2512\n",
      "Epoch 32/50; Step 100/469; Loss = 18.4520\n",
      "Epoch 32/50; Step 200/469; Loss = 22.3221\n",
      "Epoch 32/50; Step 300/469; Loss = 23.0912\n",
      "Epoch 32/50; Step 400/469; Loss = 24.8280\n",
      "Epoch 33/50; Step 100/469; Loss = 24.9554\n",
      "Epoch 33/50; Step 200/469; Loss = 26.2826\n",
      "Epoch 33/50; Step 300/469; Loss = 24.4241\n",
      "Epoch 33/50; Step 400/469; Loss = 23.8222\n",
      "Epoch 34/50; Step 100/469; Loss = 21.1435\n",
      "Epoch 34/50; Step 200/469; Loss = 26.1072\n",
      "Epoch 34/50; Step 300/469; Loss = 23.2233\n",
      "Epoch 34/50; Step 400/469; Loss = 26.2183\n",
      "Epoch 35/50; Step 100/469; Loss = 23.9804\n",
      "Epoch 35/50; Step 200/469; Loss = 19.4497\n",
      "Epoch 35/50; Step 300/469; Loss = 26.0817\n",
      "Epoch 35/50; Step 400/469; Loss = 24.4499\n",
      "Epoch 36/50; Step 100/469; Loss = 18.4929\n",
      "Epoch 36/50; Step 200/469; Loss = 20.8107\n",
      "Epoch 36/50; Step 300/469; Loss = 26.2313\n",
      "Epoch 36/50; Step 400/469; Loss = 21.5320\n",
      "Epoch 37/50; Step 100/469; Loss = 27.0870\n",
      "Epoch 37/50; Step 200/469; Loss = 24.6754\n",
      "Epoch 37/50; Step 300/469; Loss = 24.1229\n",
      "Epoch 37/50; Step 400/469; Loss = 24.1054\n",
      "Epoch 38/50; Step 100/469; Loss = 25.6501\n",
      "Epoch 38/50; Step 200/469; Loss = 18.2202\n",
      "Epoch 38/50; Step 300/469; Loss = 17.3146\n",
      "Epoch 38/50; Step 400/469; Loss = 19.7618\n",
      "Epoch 39/50; Step 100/469; Loss = 19.4974\n",
      "Epoch 39/50; Step 200/469; Loss = 22.1989\n",
      "Epoch 39/50; Step 300/469; Loss = 21.1690\n",
      "Epoch 39/50; Step 400/469; Loss = 20.9417\n",
      "Epoch 40/50; Step 100/469; Loss = 23.7887\n",
      "Epoch 40/50; Step 200/469; Loss = 22.6040\n",
      "Epoch 40/50; Step 300/469; Loss = 24.1338\n",
      "Epoch 40/50; Step 400/469; Loss = 19.9238\n",
      "Epoch 41/50; Step 100/469; Loss = 20.2668\n",
      "Epoch 41/50; Step 200/469; Loss = 18.9949\n",
      "Epoch 41/50; Step 300/469; Loss = 18.5158\n",
      "Epoch 41/50; Step 400/469; Loss = 19.1922\n",
      "Epoch 42/50; Step 100/469; Loss = 17.1520\n",
      "Epoch 42/50; Step 200/469; Loss = 20.2025\n",
      "Epoch 42/50; Step 300/469; Loss = 19.0966\n",
      "Epoch 42/50; Step 400/469; Loss = 17.5642\n",
      "Epoch 43/50; Step 100/469; Loss = 21.7102\n",
      "Epoch 43/50; Step 200/469; Loss = 20.1776\n",
      "Epoch 43/50; Step 300/469; Loss = 19.1237\n",
      "Epoch 43/50; Step 400/469; Loss = 22.6516\n",
      "Epoch 44/50; Step 100/469; Loss = 19.8329\n",
      "Epoch 44/50; Step 200/469; Loss = 19.7058\n",
      "Epoch 44/50; Step 300/469; Loss = 22.0529\n",
      "Epoch 44/50; Step 400/469; Loss = 16.8485\n",
      "Epoch 45/50; Step 100/469; Loss = 24.0510\n",
      "Epoch 45/50; Step 200/469; Loss = 20.0784\n",
      "Epoch 45/50; Step 300/469; Loss = 19.3450\n",
      "Epoch 45/50; Step 400/469; Loss = 24.6558\n",
      "Epoch 46/50; Step 100/469; Loss = 21.7966\n",
      "Epoch 46/50; Step 200/469; Loss = 23.0015\n",
      "Epoch 46/50; Step 300/469; Loss = 22.1800\n",
      "Epoch 46/50; Step 400/469; Loss = 17.4360\n",
      "Epoch 47/50; Step 100/469; Loss = 19.3144\n",
      "Epoch 47/50; Step 200/469; Loss = 24.3999\n",
      "Epoch 47/50; Step 300/469; Loss = 20.4153\n",
      "Epoch 47/50; Step 400/469; Loss = 19.0100\n",
      "Epoch 48/50; Step 100/469; Loss = 19.4700\n",
      "Epoch 48/50; Step 200/469; Loss = 23.7085\n",
      "Epoch 48/50; Step 300/469; Loss = 21.6657\n",
      "Epoch 48/50; Step 400/469; Loss = 24.1095\n",
      "Epoch 49/50; Step 100/469; Loss = 17.3407\n",
      "Epoch 49/50; Step 200/469; Loss = 25.2234\n",
      "Epoch 49/50; Step 300/469; Loss = 22.7598\n",
      "Epoch 49/50; Step 400/469; Loss = 20.3954\n",
      "Epoch 50/50; Step 100/469; Loss = 20.9510\n",
      "Epoch 50/50; Step 200/469; Loss = 19.7339\n",
      "Epoch 50/50; Step 300/469; Loss = 18.9337\n",
      "Epoch 50/50; Step 400/469; Loss = 18.5753\n",
      "Finished Training in 1m 41s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "n_steps = len(train_loader) # Total size//batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_outputs = model1(images)\n",
    "        # loss = criterion(pred_outputs, labels)\n",
    "        loss = loss_func(pred_outputs, labels)\n",
    "\n",
    "        # back prop\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        if((i+1)%100 ==0):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}; Step {i+1}/{n_steps}; Loss = {loss.item():.4f}')\n",
    "print(f'Finished Training in {(time.time()-tic)//60:.0f}m {(time.time()-tic)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.compile(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50; Step 100/469; Loss = 2.2759\n",
      "Epoch 1/50; Step 200/469; Loss = 2.2488\n",
      "Epoch 1/50; Step 300/469; Loss = 2.2338\n",
      "Epoch 1/50; Step 400/469; Loss = 2.1886\n",
      "Epoch 2/50; Step 100/469; Loss = 2.0864\n",
      "Epoch 2/50; Step 200/469; Loss = 2.0026\n",
      "Epoch 2/50; Step 300/469; Loss = 1.9474\n",
      "Epoch 2/50; Step 400/469; Loss = 1.8998\n",
      "Epoch 3/50; Step 100/469; Loss = 1.7890\n",
      "Epoch 3/50; Step 200/469; Loss = 1.7601\n",
      "Epoch 3/50; Step 300/469; Loss = 1.6519\n",
      "Epoch 3/50; Step 400/469; Loss = 1.6208\n",
      "Epoch 4/50; Step 100/469; Loss = 1.5138\n",
      "Epoch 4/50; Step 200/469; Loss = 1.4573\n",
      "Epoch 4/50; Step 300/469; Loss = 1.4876\n",
      "Epoch 4/50; Step 400/469; Loss = 1.4565\n",
      "Epoch 5/50; Step 100/469; Loss = 1.3634\n",
      "Epoch 5/50; Step 200/469; Loss = 1.2975\n",
      "Epoch 5/50; Step 300/469; Loss = 1.2884\n",
      "Epoch 5/50; Step 400/469; Loss = 1.3088\n",
      "Epoch 6/50; Step 100/469; Loss = 1.2365\n",
      "Epoch 6/50; Step 200/469; Loss = 1.1511\n",
      "Epoch 6/50; Step 300/469; Loss = 1.1006\n",
      "Epoch 6/50; Step 400/469; Loss = 1.0672\n",
      "Epoch 7/50; Step 100/469; Loss = 1.1155\n",
      "Epoch 7/50; Step 200/469; Loss = 1.0475\n",
      "Epoch 7/50; Step 300/469; Loss = 0.9932\n",
      "Epoch 7/50; Step 400/469; Loss = 1.0148\n",
      "Epoch 8/50; Step 100/469; Loss = 0.9723\n",
      "Epoch 8/50; Step 200/469; Loss = 1.0071\n",
      "Epoch 8/50; Step 300/469; Loss = 1.0089\n",
      "Epoch 8/50; Step 400/469; Loss = 0.8889\n",
      "Epoch 9/50; Step 100/469; Loss = 0.8980\n",
      "Epoch 9/50; Step 200/469; Loss = 0.7658\n",
      "Epoch 9/50; Step 300/469; Loss = 0.7565\n",
      "Epoch 9/50; Step 400/469; Loss = 0.8633\n",
      "Epoch 10/50; Step 100/469; Loss = 0.8366\n",
      "Epoch 10/50; Step 200/469; Loss = 0.8509\n",
      "Epoch 10/50; Step 300/469; Loss = 0.8924\n",
      "Epoch 10/50; Step 400/469; Loss = 0.7240\n",
      "Epoch 11/50; Step 100/469; Loss = 0.8641\n",
      "Epoch 11/50; Step 200/469; Loss = 0.6802\n",
      "Epoch 11/50; Step 300/469; Loss = 0.7130\n",
      "Epoch 11/50; Step 400/469; Loss = 0.8150\n",
      "Epoch 12/50; Step 100/469; Loss = 0.6566\n",
      "Epoch 12/50; Step 200/469; Loss = 0.6829\n",
      "Epoch 12/50; Step 300/469; Loss = 0.5440\n",
      "Epoch 12/50; Step 400/469; Loss = 0.6804\n",
      "Epoch 13/50; Step 100/469; Loss = 0.6516\n",
      "Epoch 13/50; Step 200/469; Loss = 0.6605\n",
      "Epoch 13/50; Step 300/469; Loss = 0.6686\n",
      "Epoch 13/50; Step 400/469; Loss = 0.5487\n",
      "Epoch 14/50; Step 100/469; Loss = 0.5290\n",
      "Epoch 14/50; Step 200/469; Loss = 0.5442\n",
      "Epoch 14/50; Step 300/469; Loss = 0.5861\n",
      "Epoch 14/50; Step 400/469; Loss = 0.6305\n",
      "Epoch 15/50; Step 100/469; Loss = 0.4591\n",
      "Epoch 15/50; Step 200/469; Loss = 0.5800\n",
      "Epoch 15/50; Step 300/469; Loss = 0.5975\n",
      "Epoch 15/50; Step 400/469; Loss = 0.5103\n",
      "Epoch 16/50; Step 100/469; Loss = 0.5150\n",
      "Epoch 16/50; Step 200/469; Loss = 0.5237\n",
      "Epoch 16/50; Step 300/469; Loss = 0.5020\n",
      "Epoch 16/50; Step 400/469; Loss = 0.5656\n",
      "Epoch 17/50; Step 100/469; Loss = 0.4857\n",
      "Epoch 17/50; Step 200/469; Loss = 0.4428\n",
      "Epoch 17/50; Step 300/469; Loss = 0.4458\n",
      "Epoch 17/50; Step 400/469; Loss = 0.4513\n",
      "Epoch 18/50; Step 100/469; Loss = 0.4510\n",
      "Epoch 18/50; Step 200/469; Loss = 0.4875\n",
      "Epoch 18/50; Step 300/469; Loss = 0.4524\n",
      "Epoch 18/50; Step 400/469; Loss = 0.6156\n",
      "Epoch 19/50; Step 100/469; Loss = 0.4238\n",
      "Epoch 19/50; Step 200/469; Loss = 0.4709\n",
      "Epoch 19/50; Step 300/469; Loss = 0.4478\n",
      "Epoch 19/50; Step 400/469; Loss = 0.4803\n",
      "Epoch 20/50; Step 100/469; Loss = 0.5541\n",
      "Epoch 20/50; Step 200/469; Loss = 0.4729\n",
      "Epoch 20/50; Step 300/469; Loss = 0.3619\n",
      "Epoch 20/50; Step 400/469; Loss = 0.4247\n",
      "Epoch 21/50; Step 100/469; Loss = 0.4414\n",
      "Epoch 21/50; Step 200/469; Loss = 0.3747\n",
      "Epoch 21/50; Step 300/469; Loss = 0.3797\n",
      "Epoch 21/50; Step 400/469; Loss = 0.3203\n",
      "Epoch 22/50; Step 100/469; Loss = 0.4087\n",
      "Epoch 22/50; Step 200/469; Loss = 0.2805\n",
      "Epoch 22/50; Step 300/469; Loss = 0.3386\n",
      "Epoch 22/50; Step 400/469; Loss = 0.3949\n",
      "Epoch 23/50; Step 100/469; Loss = 0.4562\n",
      "Epoch 23/50; Step 200/469; Loss = 0.3683\n",
      "Epoch 23/50; Step 300/469; Loss = 0.3878\n",
      "Epoch 23/50; Step 400/469; Loss = 0.3372\n",
      "Epoch 24/50; Step 100/469; Loss = 0.4290\n",
      "Epoch 24/50; Step 200/469; Loss = 0.2666\n",
      "Epoch 24/50; Step 300/469; Loss = 0.4239\n",
      "Epoch 24/50; Step 400/469; Loss = 0.3475\n",
      "Epoch 25/50; Step 100/469; Loss = 0.2967\n",
      "Epoch 25/50; Step 200/469; Loss = 0.3538\n",
      "Epoch 25/50; Step 300/469; Loss = 0.2794\n",
      "Epoch 25/50; Step 400/469; Loss = 0.3681\n",
      "Epoch 26/50; Step 100/469; Loss = 0.3351\n",
      "Epoch 26/50; Step 200/469; Loss = 0.2612\n",
      "Epoch 26/50; Step 300/469; Loss = 0.4121\n",
      "Epoch 26/50; Step 400/469; Loss = 0.4395\n",
      "Epoch 27/50; Step 100/469; Loss = 0.2828\n",
      "Epoch 27/50; Step 200/469; Loss = 0.3179\n",
      "Epoch 27/50; Step 300/469; Loss = 0.2867\n",
      "Epoch 27/50; Step 400/469; Loss = 0.3094\n",
      "Epoch 28/50; Step 100/469; Loss = 0.3291\n",
      "Epoch 28/50; Step 200/469; Loss = 0.2619\n",
      "Epoch 28/50; Step 300/469; Loss = 0.2277\n",
      "Epoch 28/50; Step 400/469; Loss = 0.2900\n",
      "Epoch 29/50; Step 100/469; Loss = 0.3305\n",
      "Epoch 29/50; Step 200/469; Loss = 0.2882\n",
      "Epoch 29/50; Step 300/469; Loss = 0.2888\n",
      "Epoch 29/50; Step 400/469; Loss = 0.2383\n",
      "Epoch 30/50; Step 100/469; Loss = 0.2470\n",
      "Epoch 30/50; Step 200/469; Loss = 0.2115\n",
      "Epoch 30/50; Step 300/469; Loss = 0.3122\n",
      "Epoch 30/50; Step 400/469; Loss = 0.2791\n",
      "Epoch 31/50; Step 100/469; Loss = 0.2574\n",
      "Epoch 31/50; Step 200/469; Loss = 0.3935\n",
      "Epoch 31/50; Step 300/469; Loss = 0.3013\n",
      "Epoch 31/50; Step 400/469; Loss = 0.1894\n",
      "Epoch 32/50; Step 100/469; Loss = 0.4757\n",
      "Epoch 32/50; Step 200/469; Loss = 0.3495\n",
      "Epoch 32/50; Step 300/469; Loss = 0.2308\n",
      "Epoch 32/50; Step 400/469; Loss = 0.2909\n",
      "Epoch 33/50; Step 100/469; Loss = 0.1958\n",
      "Epoch 33/50; Step 200/469; Loss = 0.2907\n",
      "Epoch 33/50; Step 300/469; Loss = 0.3187\n",
      "Epoch 33/50; Step 400/469; Loss = 0.3257\n",
      "Epoch 34/50; Step 100/469; Loss = 0.3110\n",
      "Epoch 34/50; Step 200/469; Loss = 0.2407\n",
      "Epoch 34/50; Step 300/469; Loss = 0.2529\n",
      "Epoch 34/50; Step 400/469; Loss = 0.2260\n",
      "Epoch 35/50; Step 100/469; Loss = 0.1663\n",
      "Epoch 35/50; Step 200/469; Loss = 0.1577\n",
      "Epoch 35/50; Step 300/469; Loss = 0.2357\n",
      "Epoch 35/50; Step 400/469; Loss = 0.2370\n",
      "Epoch 36/50; Step 100/469; Loss = 0.2774\n",
      "Epoch 36/50; Step 200/469; Loss = 0.1904\n",
      "Epoch 36/50; Step 300/469; Loss = 0.3229\n",
      "Epoch 36/50; Step 400/469; Loss = 0.2306\n",
      "Epoch 37/50; Step 100/469; Loss = 0.1906\n",
      "Epoch 37/50; Step 200/469; Loss = 0.2560\n",
      "Epoch 37/50; Step 300/469; Loss = 0.1644\n",
      "Epoch 37/50; Step 400/469; Loss = 0.2109\n",
      "Epoch 38/50; Step 100/469; Loss = 0.2795\n",
      "Epoch 38/50; Step 200/469; Loss = 0.2460\n",
      "Epoch 38/50; Step 300/469; Loss = 0.2251\n",
      "Epoch 38/50; Step 400/469; Loss = 0.2921\n",
      "Epoch 39/50; Step 100/469; Loss = 0.2159\n",
      "Epoch 39/50; Step 200/469; Loss = 0.2956\n",
      "Epoch 39/50; Step 300/469; Loss = 0.2376\n",
      "Epoch 39/50; Step 400/469; Loss = 0.2951\n",
      "Epoch 40/50; Step 100/469; Loss = 0.3083\n",
      "Epoch 40/50; Step 200/469; Loss = 0.4064\n",
      "Epoch 40/50; Step 300/469; Loss = 0.2252\n",
      "Epoch 40/50; Step 400/469; Loss = 0.2339\n",
      "Epoch 41/50; Step 100/469; Loss = 0.3116\n",
      "Epoch 41/50; Step 200/469; Loss = 0.1857\n",
      "Epoch 41/50; Step 300/469; Loss = 0.2581\n",
      "Epoch 41/50; Step 400/469; Loss = 0.2527\n",
      "Epoch 42/50; Step 100/469; Loss = 0.2067\n",
      "Epoch 42/50; Step 200/469; Loss = 0.1318\n",
      "Epoch 42/50; Step 300/469; Loss = 0.2421\n",
      "Epoch 42/50; Step 400/469; Loss = 0.2358\n",
      "Epoch 43/50; Step 100/469; Loss = 0.1858\n",
      "Epoch 43/50; Step 200/469; Loss = 0.2215\n",
      "Epoch 43/50; Step 300/469; Loss = 0.2318\n",
      "Epoch 43/50; Step 400/469; Loss = 0.2117\n",
      "Epoch 44/50; Step 100/469; Loss = 0.3313\n",
      "Epoch 44/50; Step 200/469; Loss = 0.1885\n",
      "Epoch 44/50; Step 300/469; Loss = 0.1933\n",
      "Epoch 44/50; Step 400/469; Loss = 0.3365\n",
      "Epoch 45/50; Step 100/469; Loss = 0.2187\n",
      "Epoch 45/50; Step 200/469; Loss = 0.1477\n",
      "Epoch 45/50; Step 300/469; Loss = 0.1626\n",
      "Epoch 45/50; Step 400/469; Loss = 0.1789\n",
      "Epoch 46/50; Step 100/469; Loss = 0.2590\n",
      "Epoch 46/50; Step 200/469; Loss = 0.2262\n",
      "Epoch 46/50; Step 300/469; Loss = 0.1741\n",
      "Epoch 46/50; Step 400/469; Loss = 0.2031\n",
      "Epoch 47/50; Step 100/469; Loss = 0.1213\n",
      "Epoch 47/50; Step 200/469; Loss = 0.2013\n",
      "Epoch 47/50; Step 300/469; Loss = 0.2401\n",
      "Epoch 47/50; Step 400/469; Loss = 0.2032\n",
      "Epoch 48/50; Step 100/469; Loss = 0.2731\n",
      "Epoch 48/50; Step 200/469; Loss = 0.1731\n",
      "Epoch 48/50; Step 300/469; Loss = 0.1646\n",
      "Epoch 48/50; Step 400/469; Loss = 0.3039\n",
      "Epoch 49/50; Step 100/469; Loss = 0.1353\n",
      "Epoch 49/50; Step 200/469; Loss = 0.2345\n",
      "Epoch 49/50; Step 300/469; Loss = 0.1924\n",
      "Epoch 49/50; Step 400/469; Loss = 0.2222\n",
      "Epoch 50/50; Step 100/469; Loss = 0.1719\n",
      "Epoch 50/50; Step 200/469; Loss = 0.1322\n",
      "Epoch 50/50; Step 300/469; Loss = 0.2932\n",
      "Epoch 50/50; Step 400/469; Loss = 0.2432\n",
      "Finished Training in 1m 50s\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_outputs = model2(images)\n",
    "        loss = criterion(pred_outputs, labels)\n",
    "\n",
    "        # back prop\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        if((i+1)%100 ==0):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}; Step {i+1}/{n_steps}; Loss = {loss.item():.4f}')\n",
    "print(f'Finished Training in {(time.time()-tic)//60:.0f}m {(time.time()-tic)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct1 = 0\n",
    "    n_correct2 = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "        pred_outputs1 = model1(images)\n",
    "        pred_outputs2 = model2(images)\n",
    "        _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "        _, actual_preds2 = torch.max(pred_outputs2, 1) # Returns value, index\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct1 += (actual_preds1 == labels).sum().item()\n",
    "        n_correct2 += (actual_preds2 == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model = 95.12%\n",
      "Accuracy for deeper model = 93.82%\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = n_correct1/n_samples * 100\n",
    "accuracy2 = n_correct2/n_samples * 100\n",
    "print(f'Accuracy for model = {accuracy1:.2f}%')\n",
    "print(f'Accuracy for deeper model = {accuracy2:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data testing\n",
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct1 = 0\n",
    "    n_correct2 = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "        pred_outputs1 = model1(images)\n",
    "        pred_outputs2 = model2(images)\n",
    "        _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "        _, actual_preds2 = torch.max(pred_outputs2, 1) # Returns value, index\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct1 += (actual_preds1 == labels).sum().item()\n",
    "        n_correct2 += (actual_preds2 == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model = 94.98%\n",
      "Accuracy for deeper model = 94.11%\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = n_correct1/n_samples * 100\n",
    "accuracy2 = n_correct2/n_samples * 100\n",
    "print(f'Accuracy for model = {accuracy1:.2f}%')\n",
    "print(f'Accuracy for deeper model = {accuracy2:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = len(train_loader)* batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = len(train_loader)* batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training in 0m 58s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tic = time.time()\n",
    "for i, (images, labels) in enumerate(train_loader2):\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=2000,\n",
    "        n_jobs=32\n",
    "    )\n",
    "    rf.fit(images,labels)\n",
    "print(f'Finished Training in {(time.time()-tic)//60:.0f}m {(time.time()-tic)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    0    0    0    0    2    3    1    4    1]\n",
      " [   0 1123    3    3    0    2    2    0    1    1]\n",
      " [   6    0 1000    5    3    0    4    8    6    0]\n",
      " [   0    0    8  977    0    5    0    9    8    3]\n",
      " [   1    0    1    0  958    0    5    0    2   15]\n",
      " [   2    0    0    9    3  864    6    2    4    2]\n",
      " [   5    3    0    0    3    3  940    0    4    0]\n",
      " [   1    2   17    0    1    0    0  994    2   11]\n",
      " [   3    0    5    8    3    5    3    4  933   10]\n",
      " [   5    5    2    9   10    2    1    5    6  964]]\n",
      "Accuracy = 97.22%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "for images, labels in test_loader2:\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    pred = rf.predict(images)\n",
    "    cm = confusion_matrix(labels, pred)\n",
    "    print(cm)\n",
    "    print(f'Accuracy = {accuracy_score(labels, pred)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training in 3m 2s\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "tic = time.time()\n",
    "for i, (images, labels) in enumerate(train_loader2):\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    xgboost1 = xgb.XGBClassifier(\n",
    "        nthread = 8,\n",
    "    )\n",
    "    xgboost1.fit(images,labels)\n",
    "print(f'Finished Training in {(time.time()-tic)//60:.0f}m {(time.time()-tic)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    1    0    0    0    3    4    1    2    0]\n",
      " [   1 1124    2    3    0    1    3    1    0    0]\n",
      " [   5    0 1006    9    1    0    0    5    5    1]\n",
      " [   0    0    2  992    0    3    0    6    4    3]\n",
      " [   0    0    3    0  957    0    4    1    2   15]\n",
      " [   2    0    1    5    0  869    6    3    4    2]\n",
      " [   7    3    0    0    2    3  939    0    4    0]\n",
      " [   1    1   14    3    2    0    0  998    2    7]\n",
      " [   4    1    3    2    3    3    2    2  948    6]\n",
      " [   6    5    1    5    7    1    0    3    3  978]]\n",
      "Accuracy = 97.8%\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_loader2:\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    pred = xgboost1.predict(images)\n",
    "    cm = confusion_matrix(labels, pred)\n",
    "    print(cm)\n",
    "    print(f'Accuracy = {accuracy_score(labels, pred)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193f6b5c64d175a70f8bc370a8e28557b54eddf9787b8dde324aa4d68183bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
