{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.5\n",
    "std_dev = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([\n",
    "    transforms.AutoAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std_dev)\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std_dev)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='../datasets',\n",
    "    train = True,\n",
    "    transform=transform2 #download is False in default\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='../datasets',\n",
    "    train = False,\n",
    "    transform=transform2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_data,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_data,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, label):\n",
    "    img = img * std_dev + mean  # unnormalize\n",
    "    plt.imshow(img.reshape(img.shape[1],img.shape[2],img.shape[0]))\n",
    "    plt.title(f'Label {label}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfpklEQVR4nO3de3DU9f3v8dcGkuViWBogNwmQAEoRiC1KGkFEyRDS1gLS8TpTYj1QaHAEvJWOgrS/c9Jif0q1EfxNlehP8ELLpdoOHrkk/GgDFpRS2pqSnFCgkCBRsiFICORz/qBuXQnqN+zyTsLzMbMz5Lvfz+6br6tPv9nNNz7nnBMAABdZjPUAAIBLEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAiJo37598vl8+tnPfhaxxywpKZHP51NJSUnEHhNoCwgQLnnFxcXy+XzasWOH9SgXJD8/Xz6f77y3f/7zn9YjAmE6Ww8AIDK+973vKScnJ2ybc04zZ87UgAEDdPnllxtNBrSMAAEdRHZ2trKzs8O2bd26VSdOnNBdd91lNBVwfnwLDvgCTp06pQULFmjkyJEKBALq3r27rr/+em3evPm8a5588kn1799fXbt21Q033KA9e/acs897772nb3/720pISFCXLl10zTXX6De/+U3E5l65cqV8Pp/uvPPOiD0mECmcAQFfQDAY1C9/+Uvdcccdmj59uurr6/Xcc88pNzdXb7/9tq6++uqw/V988UXV19eroKBAJ0+e1M9//nPddNNN+vOf/6ykpCRJ0l/+8heNHj1al19+uX7wgx+oe/fueu211zR58mT9+te/1pQpUy5o5qamJr322mu67rrrNGDAgAt6LCAqHHCJW758uZPk/vjHP553n9OnT7vGxsawbR9++KFLSkpy3/3ud0PbqqqqnCTXtWtXd/DgwdD27du3O0lu7ty5oW3jx493w4cPdydPngxta25udtddd50bPHhwaNvmzZudJLd582ZPf6/XX3/dSXLPPPOMp3XAxcK34IAvoFOnToqLi5MkNTc364MPPtDp06d1zTXX6J133jln/8mTJ4e96T9q1ChlZWXpd7/7nSTpgw8+0KZNm3Trrbeqvr5eR48e1dGjR1VbW6vc3Fzt3bv3gj+1tnLlSsXGxurWW2+9oMcBooUAAV/QCy+8oBEjRqhLly7q1auX+vTpo9/+9reqq6s7Z9/Bgwefs+2KK67Qvn37JEkVFRVyzunRRx9Vnz59wm4LFy6UJB05cqTVsx4/flzr1q1Tbm6uevXq1erHAaKJ94CAL+Cll15Sfn6+Jk+erAcffFCJiYnq1KmTCgsLVVlZ6fnxmpubJUkPPPCAcnNzW9xn0KBBrZ537dq1fPoNbR4BAr6AX/3qV8rIyNDq1avl8/lC2z8+W/m0vXv3nrPt73//e+jDABkZGZKk2NjYc352JxJWrFihyy67TN/61rci/thApPAtOOAL6NSpk6SzP9j5se3bt6usrKzF/deuXRv2Hs7bb7+t7du3Ky8vT5KUmJiocePG6dlnn9Xhw4fPWf/++++3etb3339fGzZs0JQpU9StW7dWPw4QbZwBAf/y/PPPa/369edsv++++/TNb35Tq1ev1pQpU/SNb3xDVVVVWrZsmYYOHarjx4+fs2bQoEEaM2aMZs2apcbGRi1ZskS9evXSQw89FNqnqKhIY8aM0fDhwzV9+nRlZGSopqZGZWVlOnjwoP70pz+16u/x6quv6vTp03z7DW0eAQL+ZenSpS1uz8/PV35+vqqrq/Xss8/qzTff1NChQ/XSSy9p1apVLV4k9Dvf+Y5iYmK0ZMkSHTlyRKNGjdIvfvELpaSkhPYZOnSoduzYoUWLFqm4uFi1tbVKTEzUV77yFS1YsKDVf48VK1YoMTExKt/aAyLJ5z75PQUAAC4S3gMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMNHmfg6oublZhw4dUnx8fNglTwAA7YNzTvX19UpNTVVMzPnPc9pcgA4dOqS0tDTrMQAAF+jAgQPq27fvee9vcwGKj4+XJI3R19VZscbTAAC8Oq0mbdXvQv89P5+oBaioqEiPP/64qqurlZmZqaefflqjRo363HUff9uts2LV2UeAAKDd+df1dT7vbZSofAjh1Vdf1bx587Rw4UK98847yszMVG5u7gX9gi0AQMcSlQA98cQTmj59uu6++24NHTpUy5YtU7du3fT8889H4+kAAO1QxAN06tQp7dy5M+xKvDExMcrJyWnxd6c0NjYqGAyG3QAAHV/EA3T06FGdOXNGSUlJYduTkpJUXV19zv6FhYUKBAKhG5+AA4BLg/kPos6fP191dXWh24EDB6xHAgBcBBH/FFzv3r3VqVMn1dTUhG2vqalRcnLyOfv7/X75/f5IjwEAaOMifgYUFxenkSNHauPGjaFtzc3N2rhxo7KzsyP9dACAdioqPwc0b948TZs2Tddcc41GjRqlJUuWqKGhQXfffXc0ng4A0A5FJUC33Xab3n//fS1YsEDV1dW6+uqrtX79+nM+mAAAuHT5nHPOeohPCgaDCgQCGqdJXAkBANqh065JJVqnuro69ejR47z7mX8KDgBwaSJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYiHqDHHntMPp8v7DZkyJBIPw0AoJ3rHI0Hveqqq7Rhw4Z/P0nnqDwNAKAdi0oZOnfurOTk5Gg8NACgg4jKe0B79+5VamqqMjIydNddd2n//v3n3bexsVHBYDDsBgDo+CIeoKysLBUXF2v9+vVaunSpqqqqdP3116u+vr7F/QsLCxUIBEK3tLS0SI8EAGiDfM45F80nOHbsmPr3768nnnhC99xzzzn3NzY2qrGxMfR1MBhUWlqaxmmSOvtiozkaACAKTrsmlWid6urq1KNHj/PuF/VPB/Ts2VNXXHGFKioqWrzf7/fL7/dHewwAQBsT9Z8DOn78uCorK5WSkhLtpwIAtCMRD9ADDzyg0tJS7du3T3/4wx80ZcoUderUSXfccUeknwoA0I5F/FtwBw8e1B133KHa2lr16dNHY8aM0bZt29SnT59IPxUAoB2LeIBeeeWVSD8kAKAD4lpwAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJqP9COrTegUeu87zmz7N+4XlNJ1/r/j+kOJjoec1Pdud6XhO3Pd7zmuSyBs9rJCl2/9FWrbsYTh+q9rymc2pyFCZpWVO/3p7XHLmmu+c1yc/u9LzGfeK3LqPt4AwIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgadhs2YFm55zWD+3zf85qNk3/meY0k3RV/xvua0cXen2i09yUx8nlfJKlZrlXrLoYfVF/rec1Pkn8ThUkipzX/nL4aM9vzmuQlf/C8BtHHGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkbZhZ47Wel4z+D7va/7Xuvs8r2mtg+PjPK/pctUxz2u+lrrP8xpJuj9pg+c16Z27tOq5vFqcvMPzmuYozGEteOVpz2uSozAHLhxnQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACS5GCnXetPOiPdeATRfnefa1ct2cPpM9r/HFxbby2dquA7cP8Lxm57ynPa8JNp/0vGbQilOe16Bt4gwIAGCCAAEATHgO0JYtW3TzzTcrNTVVPp9Pa9euDbvfOacFCxYoJSVFXbt2VU5Ojvbu3RupeQEAHYTnADU0NCgzM1NFRUUt3r948WI99dRTWrZsmbZv367u3bsrNzdXJ096/14vAKDj8vwhhLy8POXl5bV4n3NOS5Ys0SOPPKJJkyZJkl588UUlJSVp7dq1uv322y9sWgBAhxHR94CqqqpUXV2tnJyc0LZAIKCsrCyVlZW1uKaxsVHBYDDsBgDo+CIaoOrqaklSUlJS2PakpKTQfZ9WWFioQCAQuqWlpUVyJABAG2X+Kbj58+errq4udDtw4ID1SACAiyCiAUpOTpYk1dTUhG2vqakJ3fdpfr9fPXr0CLsBADq+iAYoPT1dycnJ2rhxY2hbMBjU9u3blZ2dHcmnAgC0c54/BXf8+HFVVFSEvq6qqtKuXbuUkJCgfv36ac6cOfqP//gPDR48WOnp6Xr00UeVmpqqyZMnR3JuAEA75zlAO3bs0I033hj6et68eZKkadOmqbi4WA899JAaGho0Y8YMHTt2TGPGjNH69evVpUuXyE0NAGj3fM45Zz3EJwWDQQUCAY3TJHX2dbyLPAIXky82rlXr0rZ6/3dvWd//8bxm6NZ8z2sG3Lbb8xpcXKddk0q0TnV1dZ/5vr75p+AAAJcmAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmPD86xgAtB9NY4e3at0zff/L85rmVjxP8n/za1ouZZwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBgp0E74/H7Pa04++GEUJmnZde/e4XlNwhtvR2EStBecAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgYKdBONI8c4nlNyfDnW/Vcu0+d8bzmS/+7a6ueC5cuzoAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABNcjBRoJ+L+zxHPa2Lka9Vz/eehXM9rfGV/atVz4dLFGRAAwAQBAgCY8BygLVu26Oabb1Zqaqp8Pp/Wrl0bdn9+fr58Pl/YbeLEiZGaFwDQQXgOUENDgzIzM1VUVHTefSZOnKjDhw+Hbi+//PIFDQkA6Hg8fwghLy9PeXl5n7mP3+9XcnJyq4cCAHR8UXkPqKSkRImJibryyis1a9Ys1dbWnnffxsZGBYPBsBsAoOOLeIAmTpyoF198URs3btRPf/pTlZaWKi8vT2fOtPw75gsLCxUIBEK3tLS0SI8EAGiDIv5zQLfffnvoz8OHD9eIESM0cOBAlZSUaPz48efsP3/+fM2bNy/0dTAYJEIAcAmI+sewMzIy1Lt3b1VUVLR4v9/vV48ePcJuAICOL+oBOnjwoGpra5WSkhLtpwIAtCOevwV3/PjxsLOZqqoq7dq1SwkJCUpISNCiRYs0depUJScnq7KyUg899JAGDRqk3Fzvl/YAAHRcngO0Y8cO3XjjjaGvP37/Ztq0aVq6dKl2796tF154QceOHVNqaqomTJigH//4x/L7/ZGbGgDQ7nkO0Lhx4+ScO+/9b7755gUNBFwKfF+5yvOadYP/2/OaD5s/8rxGkv7f0is9rwloW6ueC5curgUHADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAExH/ldwAPt9lT9VclOdZUD2+VesCL3Fla0QfZ0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkuRgpcIN9XrvK85pkB/+V5TbDZ53nNrp9d7XmNJMWLi5Ei+jgDAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDFS4AK9/6Mmz2t6xXT1vOb+6lGe18S/ykVF0XZxBgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBipMAnuOxMz2vWZRZ5XtMs7xcj/evsqzyv8elPntcAFwtnQAAAEwQIAGDCU4AKCwt17bXXKj4+XomJiZo8ebLKy8vD9jl58qQKCgrUq1cvXXbZZZo6dapqamoiOjQAoP3zFKDS0lIVFBRo27Zteuutt9TU1KQJEyaooaEhtM/cuXP1+uuva9WqVSotLdWhQ4d0yy23RHxwAED75ulDCOvXrw/7uri4WImJidq5c6fGjh2ruro6Pffcc1q5cqVuuukmSdLy5cv15S9/Wdu2bdPXvva1yE0OAGjXLug9oLq6OklSQkKCJGnnzp1qampSTk5OaJ8hQ4aoX79+Kisra/ExGhsbFQwGw24AgI6v1QFqbm7WnDlzNHr0aA0bNkySVF1drbi4OPXs2TNs36SkJFVXV7f4OIWFhQoEAqFbWlpaa0cCALQjrQ5QQUGB9uzZo1deeeWCBpg/f77q6upCtwMHDlzQ4wEA2odW/SDq7Nmz9cYbb2jLli3q27dvaHtycrJOnTqlY8eOhZ0F1dTUKDk5ucXH8vv98vv9rRkDANCOeToDcs5p9uzZWrNmjTZt2qT09PSw+0eOHKnY2Fht3LgxtK28vFz79+9XdnZ2ZCYGAHQIns6ACgoKtHLlSq1bt07x8fGh93UCgYC6du2qQCCge+65R/PmzVNCQoJ69Oihe++9V9nZ2XwCDgAQxlOAli5dKkkaN25c2Pbly5crPz9fkvTkk08qJiZGU6dOVWNjo3Jzc/XMM89EZFgAQMfhKUDOuc/dp0uXLioqKlJRkfcLNALWDt5/xvOapE7eLyz69IeDPa/xlXFhUXQsXAsOAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJlr1G1GBti6mW7dWrZv15f+J8CQtK/q/EzyvGaRtUZgEsMMZEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggouRokM6eltmq9Z9v+fWVqzyeV4x5D/3e15z2vMKoG3jDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHFSNEhdb/jcKvWNct5XvN47VDPa84crfW8BuhoOAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMVLgAr1VM8TzmrjGf0RhEqB94QwIAGCCAAEATHgKUGFhoa699lrFx8crMTFRkydPVnl5edg+48aNk8/nC7vNnDkzokMDANo/TwEqLS1VQUGBtm3bprfeektNTU2aMGGCGhoawvabPn26Dh8+HLotXrw4okMDANo/Tx9CWL9+fdjXxcXFSkxM1M6dOzV27NjQ9m7duik5OTkyEwIAOqQLeg+orq5OkpSQkBC2fcWKFerdu7eGDRum+fPn68SJE+d9jMbGRgWDwbAbAKDja/XHsJubmzVnzhyNHj1aw4YNC22/88471b9/f6Wmpmr37t16+OGHVV5ertWrV7f4OIWFhVq0aFFrxwAAtFOtDlBBQYH27NmjrVu3hm2fMWNG6M/Dhw9XSkqKxo8fr8rKSg0cOPCcx5k/f77mzZsX+joYDCotLa21YwEA2olWBWj27Nl64403tGXLFvXt2/cz983KypIkVVRUtBggv98vv9/fmjEAAO2YpwA553TvvfdqzZo1KikpUXp6+ueu2bVrlyQpJSWlVQMCADomTwEqKCjQypUrtW7dOsXHx6u6ulqSFAgE1LVrV1VWVmrlypX6+te/rl69emn37t2aO3euxo4dqxEjRkTlLwAAaJ88BWjp0qWSzv6w6SctX75c+fn5iouL04YNG7RkyRI1NDQoLS1NU6dO1SOPPBKxgQEAHYPnb8F9lrS0NJWWll7QQACASwNXwwYu0IevX+55TZK4GjbAxUgBACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABNcjBQdkn/Cvlat+6ZGel6TpD+06rmASx1nQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEy0uWvBOeckSafVJDnjYQAAnp1Wk6R///f8fNpcgOrr6yVJW/U740kAABeivr5egUDgvPf73Ocl6iJrbm7WoUOHFB8fL5/PF3ZfMBhUWlqaDhw4oB49ehhNaI/jcBbH4SyOw1kch7PawnFwzqm+vl6pqamKiTn/Oz1t7gwoJiZGffv2/cx9evTocUm/wD7GcTiL43AWx+EsjsNZ1sfhs858PsaHEAAAJggQAMBEuwqQ3+/XwoUL5ff7rUcxxXE4i+NwFsfhLI7DWe3pOLS5DyEAAC4N7eoMCADQcRAgAIAJAgQAMEGAAAAmCBAAwES7CVBRUZEGDBigLl26KCsrS2+//bb1SBfdY489Jp/PF3YbMmSI9VhRt2XLFt18881KTU2Vz+fT2rVrw+53zmnBggVKSUlR165dlZOTo71799oMG0Wfdxzy8/PPeX1MnDjRZtgoKSws1LXXXqv4+HglJiZq8uTJKi8vD9vn5MmTKigoUK9evXTZZZdp6tSpqqmpMZo4Or7IcRg3btw5r4eZM2caTdyydhGgV199VfPmzdPChQv1zjvvKDMzU7m5uTpy5Ij1aBfdVVddpcOHD4duW7dutR4p6hoaGpSZmamioqIW71+8eLGeeuopLVu2TNu3b1f37t2Vm5urkydPXuRJo+vzjoMkTZw4Mez18fLLL1/ECaOvtLRUBQUF2rZtm9566y01NTVpwoQJamhoCO0zd+5cvf7661q1apVKS0t16NAh3XLLLYZTR94XOQ6SNH369LDXw+LFi40mPg/XDowaNcoVFBSEvj5z5oxLTU11hYWFhlNdfAsXLnSZmZnWY5iS5NasWRP6urm52SUnJ7vHH388tO3YsWPO7/e7l19+2WDCi+PTx8E556ZNm+YmTZpkMo+VI0eOOEmutLTUOXf2n31sbKxbtWpVaJ+//e1vTpIrKyuzGjPqPn0cnHPuhhtucPfdd5/dUF9Amz8DOnXqlHbu3KmcnJzQtpiYGOXk5KisrMxwMht79+5VamqqMjIydNddd2n//v3WI5mqqqpSdXV12OsjEAgoKyvrknx9lJSUKDExUVdeeaVmzZql2tpa65Giqq6uTpKUkJAgSdq5c6eamprCXg9DhgxRv379OvTr4dPH4WMrVqxQ7969NWzYMM2fP18nTpywGO+82tzVsD/t6NGjOnPmjJKSksK2JyUl6b333jOaykZWVpaKi4t15ZVX6vDhw1q0aJGuv/567dmzR/Hx8dbjmaiurpakFl8fH993qZg4caJuueUWpaenq7KyUj/84Q+Vl5ensrIyderUyXq8iGtubtacOXM0evRoDRs2TNLZ10NcXJx69uwZtm9Hfj20dBwk6c4771T//v2Vmpqq3bt36+GHH1Z5eblWr15tOG24Nh8g/FteXl7ozyNGjFBWVpb69++v1157Tffcc4/hZGgLbr/99tCfhw8frhEjRmjgwIEqKSnR+PHjDSeLjoKCAu3Zs+eSeB/0s5zvOMyYMSP05+HDhyslJUXjx49XZWWlBg4ceLHHbFGb/xZc79691alTp3M+xVJTU6Pk5GSjqdqGnj176oorrlBFRYX1KGY+fg3w+jhXRkaGevfu3SFfH7Nnz9Ybb7yhzZs3h/3+sOTkZJ06dUrHjh0L27+jvh7OdxxakpWVJUlt6vXQ5gMUFxenkSNHauPGjaFtzc3N2rhxo7Kzsw0ns3f8+HFVVlYqJSXFehQz6enpSk5ODnt9BINBbd++/ZJ/fRw8eFC1tbUd6vXhnNPs2bO1Zs0abdq0Senp6WH3jxw5UrGxsWGvh/Lycu3fv79DvR4+7zi0ZNeuXZLUtl4P1p+C+CJeeeUV5/f7XXFxsfvrX//qZsyY4Xr27Omqq6utR7uo7r//fldSUuKqqqrc73//e5eTk+N69+7tjhw5Yj1aVNXX17t3333Xvfvuu06Se+KJJ9y7777r/vGPfzjnnPvJT37ievbs6datW+d2797tJk2a5NLT091HH31kPHlkfdZxqK+vdw888IArKytzVVVVbsOGDe6rX/2qGzx4sDt58qT16BEza9YsFwgEXElJiTt8+HDoduLEidA+M2fOdP369XObNm1yO3bscNnZ2S47O9tw6sj7vONQUVHhfvSjH7kdO3a4qqoqt27dOpeRkeHGjh1rPHm4dhEg55x7+umnXb9+/VxcXJwbNWqU27Ztm/VIF91tt93mUlJSXFxcnLv88svdbbfd5ioqKqzHirrNmzc7Sefcpk2b5pw7+1HsRx991CUlJTm/3+/Gjx/vysvLbYeOgs86DidOnHATJkxwffr0cbGxsa5///5u+vTpHe5/0lr6+0tyy5cvD+3z0Ucfue9///vuS1/6kuvWrZubMmWKO3z4sN3QUfB5x2H//v1u7NixLiEhwfn9fjdo0CD34IMPurq6OtvBP4XfBwQAMNHm3wMCAHRMBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPx/bBv2KJjbH6QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = next(iter(train_loader))\n",
    "images, labels = dataiter\n",
    "imshow(images[2],labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInceptionNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyInceptionNetwork, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1,16,3,1,'same')\n",
    "        self.conv12 = nn.Conv2d(1,20,5,1,'same')\n",
    "        self.conv13 = nn.Conv2d(1,12,7,1,'same')\n",
    "\n",
    "        # Adding a Batch norm at every point, brings a HUGE IMPROVEMENT in ACCURACY \n",
    "        self.norm11 = nn.BatchNorm2d(12+20+16)\n",
    "        self.conv1 = nn.Conv2d(12+20+16, 12+20+16, 1, 1) # 28, 28, 48\n",
    "        self.norm12 = nn.BatchNorm2d(12+20+16)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(12+20+16, 64, 3, 2, 1)\n",
    "        self.conv22 = nn.Conv2d(12+20+16, 32, 5, 2, 2)\n",
    "        self.conv23 = nn.Conv2d(12+20+16, 16, 7, 2, 3)\n",
    "\n",
    "        self.conv1to2 = nn.Conv2d(1, 64+32+16, 5, 2, 2)\n",
    "        self.norm1to2 = nn.BatchNorm2d(64+32+16)\n",
    "\n",
    "        self.norm21 = nn.BatchNorm2d(64+32+16)\n",
    "        self.conv2 = nn.Conv2d(64+32+16, 64+32+16, 1, 1) # 14, 14, 112\n",
    "        self.norm22 = nn.BatchNorm2d(64+32+16)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(64+32+16, 100, 1, 1, 0)\n",
    "        self.conv32 = nn.Conv2d(64+32+16, 100, 3, 1, 1)\n",
    "        self.conv33 = nn.Conv2d(64+32+16, 50, 5, 1, 2)\n",
    "        self.conv34 = nn.Conv2d(64+32+16, 50, 7, 1, 3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3,1,padding = 1) # Stride = 1\n",
    "        self.conv_maxpool = nn.Conv2d(64+32+16, 32, 1, 1, 0)\n",
    "\n",
    "        self.conv1to3 = nn.Conv2d(1, 332, 5, 2, 2)\n",
    "        self.norm1to3 = nn.BatchNorm2d(332)\n",
    "\n",
    "        self.norm31 = nn.BatchNorm2d(332)\n",
    "        self.conv3 = nn.Conv2d(332, 332, 7, 2) # 4, 4, 332\n",
    "        self.norm32 = nn.BatchNorm2d(332)\n",
    "\n",
    "        self.conv1to4 = nn.Conv2d(1, 332, 9, 5, 0)\n",
    "        self.norm1to4 = nn.BatchNorm2d(332)\n",
    "\n",
    "        self.maxpool_final = nn.MaxPool2d(2,2) # 2, 2, 332\n",
    "\n",
    "        self.linear1 = nn.Linear(332*4,500)\n",
    "        self.norm4 = nn.BatchNorm1d(500)\n",
    "        self.linear2 = nn.Linear(500,100)\n",
    "        self.norm5 = nn.BatchNorm1d(100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_temp = x\n",
    "\n",
    "        x1 = torch.relu(self.conv11(x))\n",
    "        x2 = torch.relu(self.conv12(x))\n",
    "        x3 = torch.relu(self.conv13(x))\n",
    "        x = x1\n",
    "        x = torch.cat((x,x2),1)\n",
    "        x = torch.cat((x,x3),1)\n",
    "\n",
    "        x = self.norm12(torch.relu(self.conv1(self.norm11(x))))\n",
    "\n",
    "        x1 = torch.relu(self.conv21(x))\n",
    "        x2 = torch.relu(self.conv22(x))\n",
    "        x3 = torch.relu(self.conv23(x))\n",
    "        x = x1\n",
    "        x = torch.cat((x,x2),1)\n",
    "        x = torch.cat((x,x3),1)\n",
    "\n",
    "        x = self.norm22(torch.relu(self.conv2(self.norm21(x) + self.norm1to2(torch.relu(self.conv1to2(x_temp))))))\n",
    "\n",
    "        x1 = torch.relu(self.conv31(x))\n",
    "        x2 = torch.relu(self.conv32(x))\n",
    "        x3 = torch.relu(self.conv33(x))\n",
    "        x4 = torch.relu(self.conv34(x))\n",
    "        x5 = torch.relu(self.conv_maxpool(self.maxpool1(x)))\n",
    "        x = x1\n",
    "        x = torch.cat((x,x2),1)\n",
    "        x = torch.cat((x,x3),1)\n",
    "        x = torch.cat((x,x4),1)\n",
    "        x = torch.cat((x,x5),1)\n",
    "\n",
    "        x = self.maxpool_final(self.norm32(torch.relu(self.conv3(self.norm31(x) + self.norm1to3(torch.relu(self.conv1to3(x_temp)))))) + self.norm1to4(torch.relu(self.conv1to4(x_temp))))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.norm4(torch.relu(self.linear1(x)))\n",
    "        x = self.norm5(torch.relu(self.linear2(x)))\n",
    "        x = torch.relu(self.linear3(x))\n",
    "\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "incresnet = MyInceptionNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(incresnet.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 6, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "            Conv2d-2           [-1, 20, 28, 28]             520\n",
      "            Conv2d-3           [-1, 12, 28, 28]             600\n",
      "       BatchNorm2d-4           [-1, 48, 28, 28]              96\n",
      "            Conv2d-5           [-1, 48, 28, 28]           2,352\n",
      "       BatchNorm2d-6           [-1, 48, 28, 28]              96\n",
      "            Conv2d-7           [-1, 64, 14, 14]          27,712\n",
      "            Conv2d-8           [-1, 32, 14, 14]          38,432\n",
      "            Conv2d-9           [-1, 16, 14, 14]          37,648\n",
      "      BatchNorm2d-10          [-1, 112, 14, 14]             224\n",
      "           Conv2d-11          [-1, 112, 14, 14]           2,912\n",
      "      BatchNorm2d-12          [-1, 112, 14, 14]             224\n",
      "           Conv2d-13          [-1, 112, 14, 14]          12,656\n",
      "      BatchNorm2d-14          [-1, 112, 14, 14]             224\n",
      "           Conv2d-15          [-1, 100, 14, 14]          11,300\n",
      "           Conv2d-16          [-1, 100, 14, 14]         100,900\n",
      "           Conv2d-17           [-1, 50, 14, 14]         140,050\n",
      "           Conv2d-18           [-1, 50, 14, 14]         274,450\n",
      "        MaxPool2d-19          [-1, 112, 14, 14]               0\n",
      "           Conv2d-20           [-1, 32, 14, 14]           3,616\n",
      "      BatchNorm2d-21          [-1, 332, 14, 14]             664\n",
      "           Conv2d-22          [-1, 332, 14, 14]           8,632\n",
      "      BatchNorm2d-23          [-1, 332, 14, 14]             664\n",
      "           Conv2d-24            [-1, 332, 4, 4]       5,401,308\n",
      "      BatchNorm2d-25            [-1, 332, 4, 4]             664\n",
      "           Conv2d-26            [-1, 332, 4, 4]          27,224\n",
      "      BatchNorm2d-27            [-1, 332, 4, 4]             664\n",
      "        MaxPool2d-28            [-1, 332, 2, 2]               0\n",
      "           Linear-29                  [-1, 500]         664,500\n",
      "      BatchNorm1d-30                  [-1, 500]           1,000\n",
      "           Linear-31                  [-1, 100]          50,100\n",
      "      BatchNorm1d-32                  [-1, 100]             200\n",
      "           Linear-33                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 6,810,802\n",
      "Trainable params: 6,810,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 4.49\n",
      "Params size (MB): 25.98\n",
      "Estimated Total Size (MB): 30.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(copy.deepcopy(incresnet).to('cpu'), (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30; Loss = 0.031976; LR = [0.001]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 98.32%\n",
      "--------------------\n",
      "Epoch 2/30; Loss = 0.030722; LR = [0.001]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 98.69%\n",
      "--------------------\n",
      "Epoch 3/30; Loss = 0.004341; LR = [0.001]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 99.11%\n",
      "--------------------\n",
      "Epoch 4/30; Loss = 0.001211; LR = [0.001]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 99.32%\n",
      "--------------------\n",
      "Epoch 5/30; Loss = 0.054655; LR = [0.001]\n",
      "Dev Accuracy: 99.09%\n",
      "--------------------\n",
      "Epoch 6/30; Loss = 0.001523; LR = [0.0005]\n",
      "Dev Accuracy: 98.86%\n",
      "--------------------\n",
      "Epoch 7/30; Loss = 0.000384; LR = [0.0005]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 99.35%\n",
      "--------------------\n",
      "Epoch 8/30; Loss = 0.000256; LR = [0.0005]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 99.39%\n",
      "--------------------\n",
      "Epoch 9/30; Loss = 0.000406; LR = [0.0005]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 99.42%\n",
      "--------------------\n",
      "Epoch 10/30; Loss = 0.000311; LR = [0.0005]\n",
      "SAVED MODEL WEIGHTS\n",
      "Train Accuracy: 100.00%\n",
      "Dev Accuracy: 99.44%\n",
      "--------------------\n",
      "Epoch 11/30; Loss = 0.000265; LR = [0.0005]\n",
      "Dev Accuracy: 99.43%\n",
      "--------------------\n",
      "Epoch 12/30; Loss = 0.000412; LR = [0.00025]\n",
      "SAVED MODEL WEIGHTS\n",
      "Dev Accuracy: 99.47%\n",
      "--------------------\n",
      "Epoch 13/30; Loss = 0.000212; LR = [0.00025]\n",
      "Dev Accuracy: 99.44%\n",
      "--------------------\n",
      "Epoch 14/30; Loss = 0.000199; LR = [0.00025]\n",
      "Dev Accuracy: 99.46%\n",
      "--------------------\n",
      "Epoch 15/30; Loss = 0.000139; LR = [0.00025]\n",
      "Dev Accuracy: 99.45%\n",
      "--------------------\n",
      "Epoch 16/30; Loss = 0.000111; LR = [0.00025]\n",
      "Dev Accuracy: 99.44%\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/19/ltb_35_s78j73zmsywmqs_wc0000gn/T/ipykernel_91342/3694008922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mincresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_weights = copy.deepcopy(incresnet.state_dict())\n",
    "max = 0\n",
    "val_acc = 0\n",
    "train_acc = 0\n",
    "tic = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    incresnet.train()\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = incresnet.forward(images)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step() # Decaying learning rate per 25 epochs by 0.2 times\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}; Loss = {loss.item():.6f}; LR = {scheduler.get_last_lr()}')\n",
    "    with torch.no_grad():\n",
    "        n_samples = 0\n",
    "        n_correct = 0\n",
    "        incresnet.eval()\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "            labels = labels.to(device)\n",
    "            pred_outputs1 = incresnet(images)\n",
    "            _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "            n_samples += labels.shape[0]\n",
    "            n_correct += (actual_preds1 == labels).sum().item()\n",
    "        val_acc = n_correct/n_samples * 100\n",
    "\n",
    "        if (max <= (n_correct/n_samples * 100)):\n",
    "            print('SAVED MODEL WEIGHTS')\n",
    "            max = val_acc\n",
    "            best_weights = copy.deepcopy(incresnet.state_dict())\n",
    "        \n",
    "        if((epoch+1) % 10 == 0):\n",
    "            n_samples = 0\n",
    "            n_correct = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "                labels = labels.to(device)\n",
    "                pred_outputs1 = incresnet(images)\n",
    "                _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "                n_samples += labels.shape[0]\n",
    "                n_correct += (actual_preds1 == labels).sum().item()\n",
    "            train_acc = n_correct/n_samples * 100\n",
    "            print(f'Train Accuracy: {train_acc:.2f}%')\n",
    "    print(f'Dev Accuracy: {val_acc:.2f}%')\n",
    "    print(\"-\"*20)\n",
    "print('Finished Training!')\n",
    "print(f'Best Test Accuracy = {max}%')\n",
    "print(f'Time Taken = {(time.time()-tic)//60}m {(time.time()-tic)%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement at all, performance in pure inception network is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "incresnet.load_state_dict(best_weights)\n",
    "torch.save(incresnet, 'models/simple_incresnet_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "incresnet.eval()\n",
    "for images, labels in train_loader: # Train Accuracy\n",
    "    images = images.to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.to(device)\n",
    "    pred_outputs1 = incresnet(images)\n",
    "    _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "    n_samples += labels.shape[0]\n",
    "    n_correct += (actual_preds1 == labels).sum().item()\n",
    "train_acc = n_correct/n_samples * 100\n",
    "print(f'Training Accuracy = {train_acc:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193f6b5c64d175a70f8bc370a8e28557b54eddf9787b8dde324aa4d68183bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
