{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # For Apple M1 Chip\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # For cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 100\n",
    "num_classes = 10 # 0 to 9 digits\n",
    "num_epochs = 50\n",
    "batch_size = 128 # Preferred to be a power of 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = '../datasets', \n",
    "    train = True, \n",
    "    transform = transforms.ToTensor(), \n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = '../datasets', \n",
    "    train = False, \n",
    "    transform = transforms.ToTensor(), \n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]) tensor([8, 3, 2, 9, 6, 3, 9, 7, 3, 1, 9, 4, 1, 8, 6, 5, 9, 0, 0, 3, 0, 3, 6, 2,\n",
      "        8, 7, 6, 3, 7, 0, 7, 6, 2, 9, 4, 1, 6, 0, 5, 8, 7, 1, 6, 0, 2, 3, 0, 9,\n",
      "        9, 8, 2, 1, 3, 7, 7, 3, 4, 6, 9, 3, 4, 1, 2, 6, 2, 1, 4, 2, 3, 4, 0, 8,\n",
      "        9, 2, 1, 8, 8, 1, 4, 7, 4, 0, 0, 1, 9, 6, 0, 9, 5, 9, 7, 3, 9, 1, 3, 7,\n",
      "        7, 3, 8, 4, 5, 4, 5, 5, 0, 6, 2, 1, 4, 6, 5, 4, 0, 9, 0, 5, 7, 7, 4, 6,\n",
      "        2, 9, 7, 1, 0, 3, 3, 8])\n",
      "Input and Labels shape: torch.Size([128, 1, 28, 28]) torch.Size([128])\n",
      "Unique Labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "inputs, labels = examples.next()\n",
    "print(inputs, labels)\n",
    "print('Input and Labels shape:', inputs.shape, labels.shape) # 28, 28 is the image size, 1 is for one colour channel (Not RGB but grayscale)\n",
    "print('Unique Labels:', labels.unique())                     # 128 is the label & batch size as each label is assigned a different value from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For displaying the dataset MNIST\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(inputs[i][0]) # 6 from batch-size of 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        # No softmax function here as it is already applied in the CrossEntropyLoss class in torch.nn\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden = [], num_classes=1, activations = [], seed=0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_stack = []\n",
    "        if(len(hidden) != len(activations)-1):\n",
    "            raise IndexError(\"Lengths passed for hidden and activations are not right\")\n",
    "        if(len(hidden) == 0):\n",
    "            self.linear_stack.append(nn.Linear(input_size,num_classes))\n",
    "            if(activations[0] == 'sigmoid'):\n",
    "                self.linear_stack.append(nn.Sigmoid())\n",
    "            elif(activations[0] == 'relu'):\n",
    "                self.linear_stack.append(nn.ReLU())\n",
    "            elif(activations[0] == 'tanh'):\n",
    "                self.linear_stack.append(nn.Tanh())\n",
    "            elif(activations[0] == 'softplus'):\n",
    "                self.linear_stack.append(nn.Softplus())\n",
    "        else:\n",
    "            self.linear_stack.append(nn.Linear(input_size,hidden[0]))\n",
    "            if(activations[0] == 'sigmoid'):\n",
    "                self.linear_stack.append(nn.Sigmoid())\n",
    "            elif(activations[0] == 'relu'):\n",
    "                self.linear_stack.append(nn.ReLU())\n",
    "            elif(activations[0] == 'tanh'):\n",
    "                self.linear_stack.append(nn.Tanh())\n",
    "            elif(activations[0] == 'softplus'):\n",
    "                self.linear_stack.append(nn.Softplus())\n",
    "\n",
    "            for i in range(len(hidden)-1):\n",
    "                self.linear_stack.append(nn.Linear(hidden[i],hidden[i+1]))\n",
    "                if(activations[i+1] == 'sigmoid'):\n",
    "                    self.linear_stack.append(nn.Sigmoid())\n",
    "                elif(activations[i+1] == 'relu'):\n",
    "                    self.linear_stack.append(nn.ReLU())\n",
    "                elif(activations[i+1] == 'tanh'):\n",
    "                    self.linear_stack.append(nn.Tanh())\n",
    "                elif(activations[i+1] == 'softplus'):\n",
    "                    self.linear_stack.append(nn.Softplus())\n",
    "            \n",
    "            self.linear_stack.append(nn.Linear(hidden[-1],num_classes))\n",
    "            if(activations[-1] == 'sigmoid'):\n",
    "                self.linear_stack.append(nn.Sigmoid())\n",
    "            elif(activations[-1] == 'relu'):\n",
    "                self.linear_stack.append(nn.ReLU())\n",
    "            elif(activations[-1] == 'tanh'):\n",
    "                self.linear_stack.append(nn.Tanh())\n",
    "            elif(activations[-1] == 'softplus'):\n",
    "                self.linear_stack.append(nn.Softplus())\n",
    "        self.neural_net = nn.Sequential(*self.linear_stack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.neural_net(x)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "model2 = NeuralNet2(input_size = input_size, hidden = [100, 80, 50], num_classes = num_classes, activations = ['relu', 'tanh', 'tanh', None]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSELoss(y_hat, y_true):\n",
    "    return torch.sum((y_hat - F.one_hot(y_true))**2)/len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-5)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1071906f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 100]          78,500\n",
      "              ReLU-2                  [-1, 100]               0\n",
      "            Linear-3                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(copy.deepcopy(model1).to('cpu'), (28*28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 100]          78,500\n",
      "              ReLU-3                  [-1, 100]               0\n",
      "            Linear-4                   [-1, 80]           8,080\n",
      "              Tanh-5                   [-1, 80]               0\n",
      "            Linear-6                   [-1, 50]           4,050\n",
      "              Tanh-7                   [-1, 50]               0\n",
      "            Linear-8                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 91,140\n",
      "Trainable params: 91,140\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 0.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(copy.deepcopy(model2).to('cpu'), (28*28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50; Step 100/469; Loss = 0.9408\n",
      "Epoch 1/50; Step 200/469; Loss = 0.8401\n",
      "Epoch 1/50; Step 300/469; Loss = 0.7690\n",
      "Epoch 1/50; Step 400/469; Loss = 0.7398\n",
      "Epoch 2/50; Step 100/469; Loss = 0.6358\n",
      "Epoch 2/50; Step 200/469; Loss = 0.5775\n",
      "Epoch 2/50; Step 300/469; Loss = 0.5654\n",
      "Epoch 2/50; Step 400/469; Loss = 0.5306\n",
      "Epoch 3/50; Step 100/469; Loss = 0.5140\n",
      "Epoch 3/50; Step 200/469; Loss = 0.4907\n",
      "Epoch 3/50; Step 300/469; Loss = 0.4560\n",
      "Epoch 3/50; Step 400/469; Loss = 0.4660\n",
      "Epoch 4/50; Step 100/469; Loss = 0.4611\n",
      "Epoch 4/50; Step 200/469; Loss = 0.3912\n",
      "Epoch 4/50; Step 300/469; Loss = 0.4397\n",
      "Epoch 4/50; Step 400/469; Loss = 0.4267\n",
      "Epoch 5/50; Step 100/469; Loss = 0.3849\n",
      "Epoch 5/50; Step 200/469; Loss = 0.3643\n",
      "Epoch 5/50; Step 300/469; Loss = 0.3899\n",
      "Epoch 5/50; Step 400/469; Loss = 0.3603\n",
      "Epoch 6/50; Step 100/469; Loss = 0.4126\n",
      "Epoch 6/50; Step 200/469; Loss = 0.4041\n",
      "Epoch 6/50; Step 300/469; Loss = 0.3869\n",
      "Epoch 6/50; Step 400/469; Loss = 0.3948\n",
      "Epoch 7/50; Step 100/469; Loss = 0.3485\n",
      "Epoch 7/50; Step 200/469; Loss = 0.3527\n",
      "Epoch 7/50; Step 300/469; Loss = 0.3437\n",
      "Epoch 7/50; Step 400/469; Loss = 0.3458\n",
      "Epoch 8/50; Step 100/469; Loss = 0.3554\n",
      "Epoch 8/50; Step 200/469; Loss = 0.3550\n",
      "Epoch 8/50; Step 300/469; Loss = 0.3109\n",
      "Epoch 8/50; Step 400/469; Loss = 0.3434\n",
      "Epoch 9/50; Step 100/469; Loss = 0.3207\n",
      "Epoch 9/50; Step 200/469; Loss = 0.3178\n",
      "Epoch 9/50; Step 300/469; Loss = 0.3463\n",
      "Epoch 9/50; Step 400/469; Loss = 0.2762\n",
      "Epoch 10/50; Step 100/469; Loss = 0.3307\n",
      "Epoch 10/50; Step 200/469; Loss = 0.2907\n",
      "Epoch 10/50; Step 300/469; Loss = 0.2846\n",
      "Epoch 10/50; Step 400/469; Loss = 0.3118\n",
      "Epoch 11/50; Step 100/469; Loss = 0.2510\n",
      "Epoch 11/50; Step 200/469; Loss = 0.2988\n",
      "Epoch 11/50; Step 300/469; Loss = 0.2964\n",
      "Epoch 11/50; Step 400/469; Loss = 0.3224\n",
      "Epoch 12/50; Step 100/469; Loss = 0.2773\n",
      "Epoch 12/50; Step 200/469; Loss = 0.2560\n",
      "Epoch 12/50; Step 300/469; Loss = 0.2819\n",
      "Epoch 12/50; Step 400/469; Loss = 0.2494\n",
      "Epoch 13/50; Step 100/469; Loss = 0.2719\n",
      "Epoch 13/50; Step 200/469; Loss = 0.2856\n",
      "Epoch 13/50; Step 300/469; Loss = 0.2865\n",
      "Epoch 13/50; Step 400/469; Loss = 0.2791\n",
      "Epoch 14/50; Step 100/469; Loss = 0.2601\n",
      "Epoch 14/50; Step 200/469; Loss = 0.2549\n",
      "Epoch 14/50; Step 300/469; Loss = 0.2378\n",
      "Epoch 14/50; Step 400/469; Loss = 0.2451\n",
      "Epoch 15/50; Step 100/469; Loss = 0.2577\n",
      "Epoch 15/50; Step 200/469; Loss = 0.2432\n",
      "Epoch 15/50; Step 300/469; Loss = 0.2711\n",
      "Epoch 15/50; Step 400/469; Loss = 0.2854\n",
      "Epoch 16/50; Step 100/469; Loss = 0.2499\n",
      "Epoch 16/50; Step 200/469; Loss = 0.2349\n",
      "Epoch 16/50; Step 300/469; Loss = 0.2441\n",
      "Epoch 16/50; Step 400/469; Loss = 0.2130\n",
      "Epoch 17/50; Step 100/469; Loss = 0.2200\n",
      "Epoch 17/50; Step 200/469; Loss = 0.2080\n",
      "Epoch 17/50; Step 300/469; Loss = 0.1859\n",
      "Epoch 17/50; Step 400/469; Loss = 0.2272\n",
      "Epoch 18/50; Step 100/469; Loss = 0.2663\n",
      "Epoch 18/50; Step 200/469; Loss = 0.2477\n",
      "Epoch 18/50; Step 300/469; Loss = 0.2379\n",
      "Epoch 18/50; Step 400/469; Loss = 0.2644\n",
      "Epoch 19/50; Step 100/469; Loss = 0.2201\n",
      "Epoch 19/50; Step 200/469; Loss = 0.2158\n",
      "Epoch 19/50; Step 300/469; Loss = 0.2495\n",
      "Epoch 19/50; Step 400/469; Loss = 0.2129\n",
      "Epoch 20/50; Step 100/469; Loss = 0.2598\n",
      "Epoch 20/50; Step 200/469; Loss = 0.2569\n",
      "Epoch 20/50; Step 300/469; Loss = 0.2175\n",
      "Epoch 20/50; Step 400/469; Loss = 0.1859\n",
      "Epoch 21/50; Step 100/469; Loss = 0.2353\n",
      "Epoch 21/50; Step 200/469; Loss = 0.1964\n",
      "Epoch 21/50; Step 300/469; Loss = 0.2540\n",
      "Epoch 21/50; Step 400/469; Loss = 0.2245\n",
      "Epoch 22/50; Step 100/469; Loss = 0.1732\n",
      "Epoch 22/50; Step 200/469; Loss = 0.2168\n",
      "Epoch 22/50; Step 300/469; Loss = 0.2081\n",
      "Epoch 22/50; Step 400/469; Loss = 0.2256\n",
      "Epoch 23/50; Step 100/469; Loss = 0.2063\n",
      "Epoch 23/50; Step 200/469; Loss = 0.2146\n",
      "Epoch 23/50; Step 300/469; Loss = 0.2206\n",
      "Epoch 23/50; Step 400/469; Loss = 0.1692\n",
      "Epoch 24/50; Step 100/469; Loss = 0.2439\n",
      "Epoch 24/50; Step 200/469; Loss = 0.2539\n",
      "Epoch 24/50; Step 300/469; Loss = 0.1999\n",
      "Epoch 24/50; Step 400/469; Loss = 0.1734\n",
      "Epoch 25/50; Step 100/469; Loss = 0.2149\n",
      "Epoch 25/50; Step 200/469; Loss = 0.2394\n",
      "Epoch 25/50; Step 300/469; Loss = 0.2236\n",
      "Epoch 25/50; Step 400/469; Loss = 0.1904\n",
      "Epoch 26/50; Step 100/469; Loss = 0.1960\n",
      "Epoch 26/50; Step 200/469; Loss = 0.1798\n",
      "Epoch 26/50; Step 300/469; Loss = 0.1890\n",
      "Epoch 26/50; Step 400/469; Loss = 0.1766\n",
      "Epoch 27/50; Step 100/469; Loss = 0.1646\n",
      "Epoch 27/50; Step 200/469; Loss = 0.2175\n",
      "Epoch 27/50; Step 300/469; Loss = 0.2061\n",
      "Epoch 27/50; Step 400/469; Loss = 0.1831\n",
      "Epoch 28/50; Step 100/469; Loss = 0.2187\n",
      "Epoch 28/50; Step 200/469; Loss = 0.1555\n",
      "Epoch 28/50; Step 300/469; Loss = 0.2013\n",
      "Epoch 28/50; Step 400/469; Loss = 0.2130\n",
      "Epoch 29/50; Step 100/469; Loss = 0.1730\n",
      "Epoch 29/50; Step 200/469; Loss = 0.2027\n",
      "Epoch 29/50; Step 300/469; Loss = 0.1860\n",
      "Epoch 29/50; Step 400/469; Loss = 0.1999\n",
      "Epoch 30/50; Step 100/469; Loss = 0.1786\n",
      "Epoch 30/50; Step 200/469; Loss = 0.2289\n",
      "Epoch 30/50; Step 300/469; Loss = 0.1636\n",
      "Epoch 30/50; Step 400/469; Loss = 0.2259\n",
      "Epoch 31/50; Step 100/469; Loss = 0.1970\n",
      "Epoch 31/50; Step 200/469; Loss = 0.1918\n",
      "Epoch 31/50; Step 300/469; Loss = 0.1755\n",
      "Epoch 31/50; Step 400/469; Loss = 0.1792\n",
      "Epoch 32/50; Step 100/469; Loss = 0.1444\n",
      "Epoch 32/50; Step 200/469; Loss = 0.1804\n",
      "Epoch 32/50; Step 300/469; Loss = 0.1731\n",
      "Epoch 32/50; Step 400/469; Loss = 0.1818\n",
      "Epoch 33/50; Step 100/469; Loss = 0.1897\n",
      "Epoch 33/50; Step 200/469; Loss = 0.2049\n",
      "Epoch 33/50; Step 300/469; Loss = 0.1820\n",
      "Epoch 33/50; Step 400/469; Loss = 0.1844\n",
      "Epoch 34/50; Step 100/469; Loss = 0.1625\n",
      "Epoch 34/50; Step 200/469; Loss = 0.2057\n",
      "Epoch 34/50; Step 300/469; Loss = 0.1764\n",
      "Epoch 34/50; Step 400/469; Loss = 0.1974\n",
      "Epoch 35/50; Step 100/469; Loss = 0.1786\n",
      "Epoch 35/50; Step 200/469; Loss = 0.1533\n",
      "Epoch 35/50; Step 300/469; Loss = 0.1991\n",
      "Epoch 35/50; Step 400/469; Loss = 0.1990\n",
      "Epoch 36/50; Step 100/469; Loss = 0.1349\n",
      "Epoch 36/50; Step 200/469; Loss = 0.1554\n",
      "Epoch 36/50; Step 300/469; Loss = 0.1954\n",
      "Epoch 36/50; Step 400/469; Loss = 0.1694\n",
      "Epoch 37/50; Step 100/469; Loss = 0.2048\n",
      "Epoch 37/50; Step 200/469; Loss = 0.1808\n",
      "Epoch 37/50; Step 300/469; Loss = 0.1830\n",
      "Epoch 37/50; Step 400/469; Loss = 0.1853\n",
      "Epoch 38/50; Step 100/469; Loss = 0.1924\n",
      "Epoch 38/50; Step 200/469; Loss = 0.1359\n",
      "Epoch 38/50; Step 300/469; Loss = 0.1364\n",
      "Epoch 38/50; Step 400/469; Loss = 0.1540\n",
      "Epoch 39/50; Step 100/469; Loss = 0.1436\n",
      "Epoch 39/50; Step 200/469; Loss = 0.1692\n",
      "Epoch 39/50; Step 300/469; Loss = 0.1666\n",
      "Epoch 39/50; Step 400/469; Loss = 0.1624\n",
      "Epoch 40/50; Step 100/469; Loss = 0.1745\n",
      "Epoch 40/50; Step 200/469; Loss = 0.1685\n",
      "Epoch 40/50; Step 300/469; Loss = 0.1921\n",
      "Epoch 40/50; Step 400/469; Loss = 0.1529\n",
      "Epoch 41/50; Step 100/469; Loss = 0.1618\n",
      "Epoch 41/50; Step 200/469; Loss = 0.1397\n",
      "Epoch 41/50; Step 300/469; Loss = 0.1451\n",
      "Epoch 41/50; Step 400/469; Loss = 0.1461\n",
      "Epoch 42/50; Step 100/469; Loss = 0.1332\n",
      "Epoch 42/50; Step 200/469; Loss = 0.1469\n",
      "Epoch 42/50; Step 300/469; Loss = 0.1374\n",
      "Epoch 42/50; Step 400/469; Loss = 0.1391\n",
      "Epoch 43/50; Step 100/469; Loss = 0.1643\n",
      "Epoch 43/50; Step 200/469; Loss = 0.1526\n",
      "Epoch 43/50; Step 300/469; Loss = 0.1445\n",
      "Epoch 43/50; Step 400/469; Loss = 0.1689\n",
      "Epoch 44/50; Step 100/469; Loss = 0.1491\n",
      "Epoch 44/50; Step 200/469; Loss = 0.1416\n",
      "Epoch 44/50; Step 300/469; Loss = 0.1649\n",
      "Epoch 44/50; Step 400/469; Loss = 0.1277\n",
      "Epoch 45/50; Step 100/469; Loss = 0.1890\n",
      "Epoch 45/50; Step 200/469; Loss = 0.1605\n",
      "Epoch 45/50; Step 300/469; Loss = 0.1500\n",
      "Epoch 45/50; Step 400/469; Loss = 0.1785\n",
      "Epoch 46/50; Step 100/469; Loss = 0.1732\n",
      "Epoch 46/50; Step 200/469; Loss = 0.1728\n",
      "Epoch 46/50; Step 300/469; Loss = 0.1652\n",
      "Epoch 46/50; Step 400/469; Loss = 0.1369\n",
      "Epoch 47/50; Step 100/469; Loss = 0.1453\n",
      "Epoch 47/50; Step 200/469; Loss = 0.1780\n",
      "Epoch 47/50; Step 300/469; Loss = 0.1628\n",
      "Epoch 47/50; Step 400/469; Loss = 0.1421\n",
      "Epoch 48/50; Step 100/469; Loss = 0.1537\n",
      "Epoch 48/50; Step 200/469; Loss = 0.1839\n",
      "Epoch 48/50; Step 300/469; Loss = 0.1648\n",
      "Epoch 48/50; Step 400/469; Loss = 0.1856\n",
      "Epoch 49/50; Step 100/469; Loss = 0.1339\n",
      "Epoch 49/50; Step 200/469; Loss = 0.1838\n",
      "Epoch 49/50; Step 300/469; Loss = 0.1799\n",
      "Epoch 49/50; Step 400/469; Loss = 0.1527\n",
      "Epoch 50/50; Step 100/469; Loss = 0.1627\n",
      "Epoch 50/50; Step 200/469; Loss = 0.1543\n",
      "Epoch 50/50; Step 300/469; Loss = 0.1428\n",
      "Epoch 50/50; Step 400/469; Loss = 0.1478\n",
      "Finished Training in 1m 35s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "n_steps = len(train_loader) # Total size//batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_outputs = model1(images)\n",
    "        # loss = criterion(pred_outputs, labels)\n",
    "        loss = MSELoss(pred_outputs, labels)\n",
    "\n",
    "        # back prop\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        if((i+1)%100 ==0):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}; Step {i+1}/{n_steps}; Loss = {loss.item():.4f}')\n",
    "print(f'Finished Training in {(time.time()-tic)//60:.0f}m {(time.time()-tic)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50; Step 100/469; Loss = 1.0539\n",
      "Epoch 1/50; Step 200/469; Loss = 0.9547\n",
      "Epoch 1/50; Step 300/469; Loss = 0.8538\n",
      "Epoch 1/50; Step 400/469; Loss = 0.8140\n",
      "Epoch 2/50; Step 100/469; Loss = 0.7468\n",
      "Epoch 2/50; Step 200/469; Loss = 0.7249\n",
      "Epoch 2/50; Step 300/469; Loss = 0.6716\n",
      "Epoch 2/50; Step 400/469; Loss = 0.6573\n",
      "Epoch 3/50; Step 100/469; Loss = 0.6001\n",
      "Epoch 3/50; Step 200/469; Loss = 0.5620\n",
      "Epoch 3/50; Step 300/469; Loss = 0.5803\n",
      "Epoch 3/50; Step 400/469; Loss = 0.5735\n",
      "Epoch 4/50; Step 100/469; Loss = 0.4995\n",
      "Epoch 4/50; Step 200/469; Loss = 0.4983\n",
      "Epoch 4/50; Step 300/469; Loss = 0.4750\n",
      "Epoch 4/50; Step 400/469; Loss = 0.5085\n",
      "Epoch 5/50; Step 100/469; Loss = 0.4782\n",
      "Epoch 5/50; Step 200/469; Loss = 0.4454\n",
      "Epoch 5/50; Step 300/469; Loss = 0.4014\n",
      "Epoch 5/50; Step 400/469; Loss = 0.3964\n",
      "Epoch 6/50; Step 100/469; Loss = 0.4103\n",
      "Epoch 6/50; Step 200/469; Loss = 0.3737\n",
      "Epoch 6/50; Step 300/469; Loss = 0.3552\n",
      "Epoch 6/50; Step 400/469; Loss = 0.3637\n",
      "Epoch 7/50; Step 100/469; Loss = 0.3458\n",
      "Epoch 7/50; Step 200/469; Loss = 0.3846\n",
      "Epoch 7/50; Step 300/469; Loss = 0.3727\n",
      "Epoch 7/50; Step 400/469; Loss = 0.3494\n",
      "Epoch 8/50; Step 100/469; Loss = 0.3314\n",
      "Epoch 8/50; Step 200/469; Loss = 0.2761\n",
      "Epoch 8/50; Step 300/469; Loss = 0.2734\n",
      "Epoch 8/50; Step 400/469; Loss = 0.3273\n",
      "Epoch 9/50; Step 100/469; Loss = 0.3131\n",
      "Epoch 9/50; Step 200/469; Loss = 0.3185\n",
      "Epoch 9/50; Step 300/469; Loss = 0.3363\n",
      "Epoch 9/50; Step 400/469; Loss = 0.2715\n",
      "Epoch 10/50; Step 100/469; Loss = 0.3231\n",
      "Epoch 10/50; Step 200/469; Loss = 0.2625\n",
      "Epoch 10/50; Step 300/469; Loss = 0.2855\n",
      "Epoch 10/50; Step 400/469; Loss = 0.3139\n",
      "Epoch 11/50; Step 100/469; Loss = 0.2468\n",
      "Epoch 11/50; Step 200/469; Loss = 0.2557\n",
      "Epoch 11/50; Step 300/469; Loss = 0.1964\n",
      "Epoch 11/50; Step 400/469; Loss = 0.2579\n",
      "Epoch 12/50; Step 100/469; Loss = 0.2493\n",
      "Epoch 12/50; Step 200/469; Loss = 0.2747\n",
      "Epoch 12/50; Step 300/469; Loss = 0.2767\n",
      "Epoch 12/50; Step 400/469; Loss = 0.2186\n",
      "Epoch 13/50; Step 100/469; Loss = 0.2192\n",
      "Epoch 13/50; Step 200/469; Loss = 0.2296\n",
      "Epoch 13/50; Step 300/469; Loss = 0.2459\n",
      "Epoch 13/50; Step 400/469; Loss = 0.2624\n",
      "Epoch 14/50; Step 100/469; Loss = 0.1961\n",
      "Epoch 14/50; Step 200/469; Loss = 0.2256\n",
      "Epoch 14/50; Step 300/469; Loss = 0.2617\n",
      "Epoch 14/50; Step 400/469; Loss = 0.2222\n",
      "Epoch 15/50; Step 100/469; Loss = 0.2321\n",
      "Epoch 15/50; Step 200/469; Loss = 0.2123\n",
      "Epoch 15/50; Step 300/469; Loss = 0.2266\n",
      "Epoch 15/50; Step 400/469; Loss = 0.2528\n",
      "Epoch 16/50; Step 100/469; Loss = 0.2380\n",
      "Epoch 16/50; Step 200/469; Loss = 0.1855\n",
      "Epoch 16/50; Step 300/469; Loss = 0.1867\n",
      "Epoch 16/50; Step 400/469; Loss = 0.1898\n",
      "Epoch 17/50; Step 100/469; Loss = 0.2112\n",
      "Epoch 17/50; Step 200/469; Loss = 0.2178\n",
      "Epoch 17/50; Step 300/469; Loss = 0.2131\n",
      "Epoch 17/50; Step 400/469; Loss = 0.2729\n",
      "Epoch 18/50; Step 100/469; Loss = 0.1834\n",
      "Epoch 18/50; Step 200/469; Loss = 0.2019\n",
      "Epoch 18/50; Step 300/469; Loss = 0.2091\n",
      "Epoch 18/50; Step 400/469; Loss = 0.2004\n",
      "Epoch 19/50; Step 100/469; Loss = 0.2615\n",
      "Epoch 19/50; Step 200/469; Loss = 0.2087\n",
      "Epoch 19/50; Step 300/469; Loss = 0.1582\n",
      "Epoch 19/50; Step 400/469; Loss = 0.2205\n",
      "Epoch 20/50; Step 100/469; Loss = 0.2223\n",
      "Epoch 20/50; Step 200/469; Loss = 0.1706\n",
      "Epoch 20/50; Step 300/469; Loss = 0.1805\n",
      "Epoch 20/50; Step 400/469; Loss = 0.1598\n",
      "Epoch 21/50; Step 100/469; Loss = 0.1995\n",
      "Epoch 21/50; Step 200/469; Loss = 0.1489\n",
      "Epoch 21/50; Step 300/469; Loss = 0.1929\n",
      "Epoch 21/50; Step 400/469; Loss = 0.1777\n",
      "Epoch 22/50; Step 100/469; Loss = 0.1966\n",
      "Epoch 22/50; Step 200/469; Loss = 0.1837\n",
      "Epoch 22/50; Step 300/469; Loss = 0.1791\n",
      "Epoch 22/50; Step 400/469; Loss = 0.1877\n",
      "Epoch 23/50; Step 100/469; Loss = 0.2027\n",
      "Epoch 23/50; Step 200/469; Loss = 0.1444\n",
      "Epoch 23/50; Step 300/469; Loss = 0.2136\n",
      "Epoch 23/50; Step 400/469; Loss = 0.2014\n",
      "Epoch 24/50; Step 100/469; Loss = 0.1427\n",
      "Epoch 24/50; Step 200/469; Loss = 0.1618\n",
      "Epoch 24/50; Step 300/469; Loss = 0.1533\n",
      "Epoch 24/50; Step 400/469; Loss = 0.1901\n",
      "Epoch 25/50; Step 100/469; Loss = 0.1594\n",
      "Epoch 25/50; Step 200/469; Loss = 0.1460\n",
      "Epoch 25/50; Step 300/469; Loss = 0.2053\n",
      "Epoch 25/50; Step 400/469; Loss = 0.2232\n",
      "Epoch 26/50; Step 100/469; Loss = 0.1572\n",
      "Epoch 26/50; Step 200/469; Loss = 0.1815\n",
      "Epoch 26/50; Step 300/469; Loss = 0.1609\n",
      "Epoch 26/50; Step 400/469; Loss = 0.1636\n",
      "Epoch 27/50; Step 100/469; Loss = 0.1836\n",
      "Epoch 27/50; Step 200/469; Loss = 0.1455\n",
      "Epoch 27/50; Step 300/469; Loss = 0.1254\n",
      "Epoch 27/50; Step 400/469; Loss = 0.1514\n",
      "Epoch 28/50; Step 100/469; Loss = 0.1851\n",
      "Epoch 28/50; Step 200/469; Loss = 0.1591\n",
      "Epoch 28/50; Step 300/469; Loss = 0.1683\n",
      "Epoch 28/50; Step 400/469; Loss = 0.1408\n",
      "Epoch 29/50; Step 100/469; Loss = 0.1285\n",
      "Epoch 29/50; Step 200/469; Loss = 0.1315\n",
      "Epoch 29/50; Step 300/469; Loss = 0.1673\n",
      "Epoch 29/50; Step 400/469; Loss = 0.1475\n",
      "Epoch 30/50; Step 100/469; Loss = 0.1393\n",
      "Epoch 30/50; Step 200/469; Loss = 0.1882\n",
      "Epoch 30/50; Step 300/469; Loss = 0.1775\n",
      "Epoch 30/50; Step 400/469; Loss = 0.1279\n",
      "Epoch 31/50; Step 100/469; Loss = 0.2070\n",
      "Epoch 31/50; Step 200/469; Loss = 0.1533\n",
      "Epoch 31/50; Step 300/469; Loss = 0.1350\n",
      "Epoch 31/50; Step 400/469; Loss = 0.1498\n",
      "Epoch 32/50; Step 100/469; Loss = 0.1261\n",
      "Epoch 32/50; Step 200/469; Loss = 0.1579\n",
      "Epoch 32/50; Step 300/469; Loss = 0.1716\n",
      "Epoch 32/50; Step 400/469; Loss = 0.1823\n",
      "Epoch 33/50; Step 100/469; Loss = 0.1795\n",
      "Epoch 33/50; Step 200/469; Loss = 0.1484\n",
      "Epoch 33/50; Step 300/469; Loss = 0.1368\n",
      "Epoch 33/50; Step 400/469; Loss = 0.1349\n",
      "Epoch 34/50; Step 100/469; Loss = 0.1256\n",
      "Epoch 34/50; Step 200/469; Loss = 0.1120\n",
      "Epoch 34/50; Step 300/469; Loss = 0.1574\n",
      "Epoch 34/50; Step 400/469; Loss = 0.1372\n",
      "Epoch 35/50; Step 100/469; Loss = 0.1672\n",
      "Epoch 35/50; Step 200/469; Loss = 0.1220\n",
      "Epoch 35/50; Step 300/469; Loss = 0.1594\n",
      "Epoch 35/50; Step 400/469; Loss = 0.1425\n",
      "Epoch 36/50; Step 100/469; Loss = 0.1247\n",
      "Epoch 36/50; Step 200/469; Loss = 0.1459\n",
      "Epoch 36/50; Step 300/469; Loss = 0.1193\n",
      "Epoch 36/50; Step 400/469; Loss = 0.1320\n",
      "Epoch 37/50; Step 100/469; Loss = 0.1395\n",
      "Epoch 37/50; Step 200/469; Loss = 0.1398\n",
      "Epoch 37/50; Step 300/469; Loss = 0.1165\n",
      "Epoch 37/50; Step 400/469; Loss = 0.1582\n",
      "Epoch 38/50; Step 100/469; Loss = 0.1258\n",
      "Epoch 38/50; Step 200/469; Loss = 0.1622\n",
      "Epoch 38/50; Step 300/469; Loss = 0.1467\n",
      "Epoch 38/50; Step 400/469; Loss = 0.1538\n",
      "Epoch 39/50; Step 100/469; Loss = 0.1613\n",
      "Epoch 39/50; Step 200/469; Loss = 0.1934\n",
      "Epoch 39/50; Step 300/469; Loss = 0.1309\n",
      "Epoch 39/50; Step 400/469; Loss = 0.1272\n",
      "Epoch 40/50; Step 100/469; Loss = 0.1672\n",
      "Epoch 40/50; Step 200/469; Loss = 0.1265\n",
      "Epoch 40/50; Step 300/469; Loss = 0.1541\n",
      "Epoch 40/50; Step 400/469; Loss = 0.1667\n",
      "Epoch 41/50; Step 100/469; Loss = 0.1164\n",
      "Epoch 41/50; Step 200/469; Loss = 0.0990\n",
      "Epoch 41/50; Step 300/469; Loss = 0.1342\n",
      "Epoch 41/50; Step 400/469; Loss = 0.1294\n",
      "Epoch 42/50; Step 100/469; Loss = 0.1077\n",
      "Epoch 42/50; Step 200/469; Loss = 0.1212\n",
      "Epoch 42/50; Step 300/469; Loss = 0.1349\n",
      "Epoch 42/50; Step 400/469; Loss = 0.1247\n",
      "Epoch 43/50; Step 100/469; Loss = 0.1800\n",
      "Epoch 43/50; Step 200/469; Loss = 0.1415\n",
      "Epoch 43/50; Step 300/469; Loss = 0.1329\n",
      "Epoch 43/50; Step 400/469; Loss = 0.1607\n",
      "Epoch 44/50; Step 100/469; Loss = 0.1235\n",
      "Epoch 44/50; Step 200/469; Loss = 0.1169\n",
      "Epoch 44/50; Step 300/469; Loss = 0.1136\n",
      "Epoch 44/50; Step 400/469; Loss = 0.1027\n",
      "Epoch 45/50; Step 100/469; Loss = 0.1395\n",
      "Epoch 45/50; Step 200/469; Loss = 0.1226\n",
      "Epoch 45/50; Step 300/469; Loss = 0.1293\n",
      "Epoch 45/50; Step 400/469; Loss = 0.1291\n",
      "Epoch 46/50; Step 100/469; Loss = 0.0932\n",
      "Epoch 46/50; Step 200/469; Loss = 0.1242\n",
      "Epoch 46/50; Step 300/469; Loss = 0.1166\n",
      "Epoch 46/50; Step 400/469; Loss = 0.1354\n",
      "Epoch 47/50; Step 100/469; Loss = 0.1410\n",
      "Epoch 47/50; Step 200/469; Loss = 0.1324\n",
      "Epoch 47/50; Step 300/469; Loss = 0.1142\n",
      "Epoch 47/50; Step 400/469; Loss = 0.1700\n",
      "Epoch 48/50; Step 100/469; Loss = 0.1088\n",
      "Epoch 48/50; Step 200/469; Loss = 0.1375\n",
      "Epoch 48/50; Step 300/469; Loss = 0.1252\n",
      "Epoch 48/50; Step 400/469; Loss = 0.1282\n",
      "Epoch 49/50; Step 100/469; Loss = 0.1205\n",
      "Epoch 49/50; Step 200/469; Loss = 0.0981\n",
      "Epoch 49/50; Step 300/469; Loss = 0.1543\n",
      "Epoch 49/50; Step 400/469; Loss = 0.1390\n",
      "Epoch 50/50; Step 100/469; Loss = 0.1141\n",
      "Epoch 50/50; Step 200/469; Loss = 0.1233\n",
      "Epoch 50/50; Step 300/469; Loss = 0.1210\n",
      "Epoch 50/50; Step 400/469; Loss = 0.1009\n",
      "Finished Training in 1m 45s\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_outputs = model2(images)\n",
    "        loss = MSELoss(pred_outputs, labels)\n",
    "\n",
    "        # back prop\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        if((i+1)%100 ==0):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}; Step {i+1}/{n_steps}; Loss = {loss.item():.4f}')\n",
    "print(f'Finished Training in {(time.time()-tic)//60:.0f}m {(time.time()-tic)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct1 = 0\n",
    "    n_correct2 = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "        pred_outputs1 = model1(images)\n",
    "        pred_outputs2 = model2(images)\n",
    "        _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "        _, actual_preds2 = torch.max(pred_outputs2, 1) # Returns value, index\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct1 += (actual_preds1 == labels).sum().item()\n",
    "        n_correct2 += (actual_preds2 == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model = 95.19%\n",
      "Accuracy for deeper model = 95.27%\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = n_correct1/n_samples * 100\n",
    "accuracy2 = n_correct2/n_samples * 100\n",
    "print(f'Accuracy for model = {accuracy1:.2f}%')\n",
    "print(f'Accuracy for deeper model = {accuracy2:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data testing\n",
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct1 = 0\n",
    "    n_correct2 = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.reshape(-1, input_size).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "        labels = labels.to(device)\n",
    "        pred_outputs1 = model1(images)\n",
    "        pred_outputs2 = model2(images)\n",
    "        _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "        _, actual_preds2 = torch.max(pred_outputs2, 1) # Returns value, index\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct1 += (actual_preds1 == labels).sum().item()\n",
    "        n_correct2 += (actual_preds2 == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model = 95.19%\n",
      "Accuracy for deeper model = 95.19%\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = n_correct1/n_samples * 100\n",
    "accuracy2 = n_correct2/n_samples * 100\n",
    "print(f'Accuracy for model = {accuracy1:.2f}%')\n",
    "print(f'Accuracy for deeper model = {accuracy2:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = len(train_loader)* batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = len(train_loader)* batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for i, (images, labels) in enumerate(train_loader2):\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=2000,\n",
    "        n_jobs=32\n",
    "    )\n",
    "    rf.fit(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    0    0    0    0    3    3    1    3    1]\n",
      " [   0 1123    3    3    0    2    2    0    1    1]\n",
      " [   6    0 1000    6    2    0    4    8    6    0]\n",
      " [   0    0   11  973    0    6    0    9    8    3]\n",
      " [   1    0    1    0  957    0    5    0    2   16]\n",
      " [   3    0    0   10    3  862    6    1    5    2]\n",
      " [   7    3    0    0    2    3  940    0    3    0]\n",
      " [   1    2   17    1    1    0    0  993    1   12]\n",
      " [   4    0    6    7    4    5    3    3  931   11]\n",
      " [   5    5    2   10   13    3    1    5    5  960]]\n",
      "Accuracy = 97.08%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "for images, labels in test_loader2:\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    pred = rf.predict(images)\n",
    "    cm = confusion_matrix(labels, pred)\n",
    "    print(cm)\n",
    "    print(f'Accuracy = {accuracy_score(labels, pred)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "for i, (images, labels) in enumerate(train_loader2):\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    xgboost1 = xgb.XGBClassifier(\n",
    "        nthread = 8,\n",
    "    )\n",
    "    xgboost1.fit(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    1    0    0    0    3    4    1    2    0]\n",
      " [   1 1124    2    3    0    1    3    1    0    0]\n",
      " [   5    0 1006    9    1    0    0    5    5    1]\n",
      " [   0    0    2  992    0    3    0    6    4    3]\n",
      " [   0    0    3    0  957    0    4    1    2   15]\n",
      " [   2    0    1    5    0  869    6    3    4    2]\n",
      " [   7    3    0    0    2    3  939    0    4    0]\n",
      " [   1    1   14    3    2    0    0  998    2    7]\n",
      " [   4    1    3    2    3    3    2    2  948    6]\n",
      " [   6    5    1    5    7    1    0    3    3  978]]\n",
      "Accuracy = 97.8%\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_loader2:\n",
    "    images = images.reshape(-1, input_size).detach().numpy() # From 128, 1, 28, 28 ---> 128, 784\n",
    "    labels = labels.detach().numpy()\n",
    "    pred = xgboost1.predict(images)\n",
    "    cm = confusion_matrix(labels, pred)\n",
    "    print(cm)\n",
    "    print(f'Accuracy = {accuracy_score(labels, pred)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193f6b5c64d175a70f8bc370a8e28557b54eddf9787b8dde324aa4d68183bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
