{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "## Attention Based Bidirectional LSTM\n",
    "\n",
    "<p align=\"center\">\n",
    "<b>Attention Model with Bidirectional LSTM</b>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<img src=\"../images/Attention Model with BiLSTM.png\" style=\"width:250px;height:450px;\">\n",
    "</p>\n",
    "\n",
    "\n",
    "The context vector $c_i$ depends on the sequence of *annotations $(h_1, h_2,...h_{T_x})$ (hidden states sequence at input)*, to which the encoder maps the input sentence. The context vector $c_i$ is, then, computed as a weighted sum of these annotations $h_i$ -\n",
    "\n",
    "$$c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_j$$\n",
    "\n",
    "The weight $\\alpha{ij}$ of each annotation $h_j$ is computed by a softmax on $e_{ij}$-\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$$\n",
    "\n",
    "where $e_{ij}$ is calculated by an *alignment model* which scores how well the inputs around position $j$ and the output at position $i$ match. as follows -\n",
    "\n",
    "$$e_{ij} = a(s_{i-1}, h_j)$$\n",
    "\n",
    "where -\n",
    "- $s_{i-1}$: The previous output's hidden state\n",
    "- $h_j$: Hidden state of $j^{th}$ input\n",
    "\n",
    "Alignment model $a(s_{i-1}, h_j)$ is modelled using a *feed-forward network*, which is jointly trained with all the other components of the system.\n",
    "\n",
    "<p align=\"center\">\n",
    "Source: <a href=\"https://arxiv.org/pdf/1409.0473.pdf\">Original Attention model paper</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter # To print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_eng = spacy.load('en')\n",
    "# spacy_ger = spacy.load('de')\n",
    "def tokenize(text):\n",
    "    p = np.array([])\n",
    "    for s in text.split(','):\n",
    "        s1 = s.split(' ')\n",
    "        for s2 in s1:\n",
    "            s2 = s2.split('.')\n",
    "            if s2!=['']:\n",
    "                p = np.append(p, s2)\n",
    "    if p[-1]=='':\n",
    "        return list(p[:-1])\n",
    "    else:\n",
    "        return list(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'today', 'is', 'the', 'day']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "tokenize(\"Hello, today is the day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = Field(\n",
    "    sequential=True, \n",
    "    use_vocab=True, \n",
    "    tokenize=tokenize, \n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>'\n",
    ")\n",
    "german = Field(\n",
    "    sequential=True, \n",
    "    use_vocab=True, \n",
    "    tokenize=tokenize, \n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = Multi30k.splits(\n",
    "    exts = ('.de', '.en'), # (Source language, Target Language)\n",
    "    fields = (german, english)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary\n",
    "english.build_vocab(train_data, max_size = 10000, min_freq = 2) # We won't add words used ONLY once, should occur atleast twice\n",
    "german.build_vocab(train_data, max_size = 10000, min_freq = 2) # We won't add words used ONLY once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `BucketIterator` -\n",
    "\n",
    "`sort_within_batch` and `sort_key` is going to prioritise to have examples of SIMILAR LENGTH in the batch, to minimize the amount of padding to save the amount of computing to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.src), # This would prioritise the similar length sentences in the batch\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_prob) -> None:\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n",
    "        \n",
    "        if num_layers == 1:\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional = True) # Added Bi-directional\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional = True, dropout=dropout_prob)\n",
    "\n",
    "        self.hidden_fc = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.cell_fc = nn.Linear(hidden_size*2, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, batch_size)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x)) # Will return the embedding of every word in x\n",
    "        # embedding shape: (seq_length, batch_size, embedding_size)\n",
    "\n",
    "        encoder_states, (hidden, cell) = self.lstm(embedding)\n",
    "        # encoder_states shape: (seq_length, batch_size, hidden_size*2)\n",
    "\n",
    "        # hidden and cell state shape: (2, batch_size, hidden_size)\n",
    "        hidden = self.hidden_fc(torch.cat((hidden[0:1], hidden[1:2]), dim = 2)) # [0:1]: Forward, [1:2]: Backward\n",
    "        cell = self.cell_fc(torch.cat((cell[0:1], cell[1:2]), dim = 2))\n",
    "        # dim = 2 since we need to concatenate in the hidden_size dimension\n",
    "\n",
    "        \n",
    "        # Context vector: hidden and cell state (These would only be the last hidden and cell state)\n",
    "        # encoder_states: will include the entire hidden state sequence (h_j)\n",
    "        # encoder_states shape: (seq_length, batch_size, hidden_size)\n",
    "        return encoder_states, hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob) -> None:\n",
    "        \"\"\"\n",
    "            input_size: Size of the vocabulary\n",
    "            output_size = input_size (Each dimension = Probability of each word)\n",
    "            hidden_size: Same size as the hidden state size of the Encoder\n",
    "        \"\"\"\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n",
    "        if num_layers == 1:\n",
    "            self.lstm = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=dropout_prob)\n",
    "        # hidden_size*2: Since Encoder is a Bidirectional LSTM, so two times the hidden_size\n",
    "        # So here the hidden_size*2: Context Vector\n",
    "        # embedding_size: Is the same as before\n",
    "\n",
    "        self.energy_alignment_model = nn.Linear(hidden_size*3, 1)\n",
    "        # First we will add hidden state from encoder (2*hidden_size), and one from decoder (hidden_size)\n",
    "        # Hence 3*hidden_size\n",
    "        self.softmax_energy = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # At the output at each iteration\n",
    "    \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        \"\"\"\n",
    "            This function predicts ONLY ONE iteration/one word at a time.\n",
    "            Will need to be iteratively called for the entire translation prediction.\n",
    "        \"\"\"\n",
    "        # Context Vector: hidden and cell state of Encoder\n",
    "        # x shape: (batch_size), but we want (1, batch_size) - 1 represents 1 word at a time, as a batch of batch_size\n",
    "        x = x.unsqueeze(0) # Will add 1 dimension\n",
    "\n",
    "        embedding_vector = self.dropout(self.embedding(x)) # Applies dropout on the embedding values for all words\n",
    "        # embedding_vector shape: (1, batch_size, embedding_size)\n",
    "\n",
    "        sequence_length = encoder_states.shape[0] # Shape of encoder_states: (sequence_length, batch_size, hidden_size*2)\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1) # Decoder hidden state\n",
    "        # So it changes its shape from: (batch_size, hidden_size) --> (sequence_length, batch_size, hidden_size)\n",
    "        # Basically, it is a repeated version of hidden, which is repeated sequence_length number of times\n",
    "        # The next torch.cat() operation converts the shape to --> (sequence_length, batch_size, hidden_size*3)\n",
    "\n",
    "        energy = self.relu(self.energy_alignment_model(torch.cat((h_reshaped, encoder_states), dim = 2)))\n",
    "        # energy shape: (sequence_length, batch_size, 1)\n",
    "        attention = self.softmax_energy(energy) # Dimension was set 0, since we needed to take softmax on encoder sequence_length dimension\n",
    "        # attention shape: (sequence_length, batch_size, 1)\n",
    "\n",
    "\n",
    "        # attention = attention.permute(1,2,0)\n",
    "        # # attention shape: (batch_size, 1, sequence_length)\n",
    "        # encoder_states = encoder_states.permute(1,0,2)\n",
    "        # # encoder_states shape: (sequence_length, batch_size, hidden_size*2) --> (batch_size, sequence_length, hidden_size*2)\n",
    "        # context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        # # context_vector shape: (batch_size, 1, hidden_size*2) --> (permuting to) (1, batch_size, hidden_size*2)\n",
    "\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "        lstm_input = torch.cat((context_vector, embedding_vector), dim = 2)\n",
    "        # lstm_input shape: (1, batch_size, hidden_size*2 + embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        # Shape of hidden_state: (1, batch_size, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # Shape of predictions: (1, batch_size, length_of_vocabulary)\n",
    "\n",
    "        predictions = predictions.squeeze(0) # To remove the dimension 1\n",
    "        # New shape: (batch_size, length_of_vocabulary)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 44, 33]), torch.Size([10, 22, 33]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To understand what repeat() is doing\n",
    "asdf = torch.rand((22,33))\n",
    "asdf.repeat(10,2,1).shape, asdf.repeat(10,1,1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The overall Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder, decoder) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio = 0.5):\n",
    "        \"\"\"\n",
    "            teacher_force_ratio -\n",
    "            Basically in the decoder, when one word is predicted,\n",
    "            we use that word as the input for prediction of te next\n",
    "            word. Here, we shall not do that completely. Instead, \n",
    "            we shall assign a probability for using the previous \n",
    "            predicted word, and the rest of the times, we shall use\n",
    "            the ground-truth word as the input.\n",
    "\n",
    "            The teacher_force_ratio determines the probability of \n",
    "            using the ground_truth word, instead of the previous \n",
    "            predicted word. We will never keep it as 1 (meaning \n",
    "            ONLY ground-truth word will be used), as that will \n",
    "            completely hamper the learning of the LSTM model.\n",
    "        \"\"\"\n",
    "        # source shape: (source_sentence_len, batch_size)\n",
    "        # target shape: (target_sentence_len, batch_size)\n",
    "        # Different lengths in one batch are padded to the length of the longest sentence\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        encoder_states, hidden_context, cell_context = self.encoder(source)\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        # Each word will have a vector of entire vocabulary size with a batch size of batch_size\n",
    "        # Each prediction will be added to target_len dimension (dimension 0)\n",
    "        \n",
    "        # Grab start token (<sos>)\n",
    "        x = target[0] # Shape = (1, batch_size)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden_context, cell_context = self.decoder(x, encoder_states, hidden_context, cell_context)\n",
    "            outputs[t] = output # adding along the first dimension\n",
    "\n",
    "            # output shape: (batch_size, english_vocab_size)\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = input_size_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "encoder_dropout = 0.0\n",
    "decoder_dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "writer = SummaryWriter(f'runs/Attention_Loss_plot')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = EncoderLSTM(\n",
    "    input_size_encoder, \n",
    "    encoder_embedding_size, \n",
    "    hidden_size, \n",
    "    num_layers, \n",
    "    encoder_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = DecoderLSTM(\n",
    "    input_size_decoder, \n",
    "    decoder_embedding_size, \n",
    "    hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout_prob=decoder_dropout\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention(encoder=encoder_net, decoder=decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi['<pad>'] # To obtain the index for <pad> token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "# Since we don't want the loss to be calculated for the padding done on shorter sentences\n",
    "# During averaging the loss part, the loss on these pad tokens won't be used for calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"models_state_dict/Seq2Seq_Attention_checkpoint.pth.tar\"):\n",
    "    print(\"Saving Checkpoint...\")\n",
    "    torch.save(state, filename)\n",
    "    print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    print(\"Successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, source, target, device, max_length=60):\n",
    "    # print(sentence)\n",
    "    # sys.exit()\n",
    "\n",
    "    # Load source tokenizer\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.lower() for token in tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # spacy_ger = spacy.load(\"de\")\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    # if type(sentence) == str:\n",
    "    #     tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    # else:\n",
    "    #     tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <SOS> and <EOS> in the beginning and end respectively\n",
    "    tokens.insert(0, source.init_token)\n",
    "    tokens.append(source.eos_token)\n",
    "\n",
    "    # Go through each source token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to tensor and add 1 dimension at the 1st index (2nd dimension)\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden and cell state\n",
    "    with torch.no_grad():\n",
    "        # Obtain context vectors for decoder (hidden and cell state)\n",
    "        encoder_states, hidden, cell = model.encoder(sentence_tensor) # Will not build computational graph\n",
    "    \n",
    "    outputs = [target.vocab.stoi[\"<sos>\"]] # First word to be inputted\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction, hidden, cell = model.decoder(previous_word, encoder_states, hidden, cell)\n",
    "            best_guess = torch.argmax(prediction, dim=1).item()\n",
    "        \n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model checks if prediction is an <eos> token or End of Sentence token\n",
    "        if outputs[-1] == target.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    translated_sentence = [target.vocab.itos[index] for index in outputs]\n",
    "\n",
    "    # Remove start token\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(data, model, source, target, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1] # Removing <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "    \n",
    "    return bleu_score(outputs, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters = 39125721\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # Number of Parameters\n",
    "    print(f'Total number of trainable parameters = {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"models_state_dict/Seq2Seq_Attention_checkpoint.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Translated Example Sentence: \n",
      " a boat with several men is being pulled by a large boat \n",
      "Time taken: 3m 30s\n",
      "Bleu Score on Validation set = 21.91\n",
      "Epoch 2/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled by horses by a large of horses \n",
      "Time taken: 3m 30s\n",
      "Bleu Score on Validation set = 22.29\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 3/30\n",
      "Translated Example Sentence: \n",
      " a boat with several men is being pulled by a large shore of horses \n",
      "Time taken: 3m 27s\n",
      "Bleu Score on Validation set = 22.34\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 4/30\n",
      "Translated Example Sentence: \n",
      " a boat with several men is pulled by a large shore of horses \n",
      "Time taken: 3m 25s\n",
      "Bleu Score on Validation set = 22.07\n",
      "Epoch 5/30\n",
      "Translated Example Sentence: \n",
      " a boat with several men is pulled to shore by a large of of horses \n",
      "Time taken: 3m 23s\n",
      "Bleu Score on Validation set = 22.46\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 6/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large of horses \n",
      "Time taken: 3m 24s\n",
      "Bleu Score on Validation set = 22.58\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 7/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore of a large boat of horses \n",
      "Time taken: 3m 23s\n",
      "Bleu Score on Validation set = 23.01\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 8/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 21s\n",
      "Bleu Score on Validation set = 22.59\n",
      "Epoch 9/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 27s\n",
      "Bleu Score on Validation set = 23.14\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 10/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 27s\n",
      "Bleu Score on Validation set = 22.98\n",
      "Epoch 11/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 25s\n",
      "Bleu Score on Validation set = 22.83\n",
      "Epoch 12/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 19s\n",
      "Bleu Score on Validation set = 22.67\n",
      "Epoch 13/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 20s\n",
      "Bleu Score on Validation set = 23.03\n",
      "Epoch 14/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 25s\n",
      "Bleu Score on Validation set = 22.86\n",
      "Epoch 15/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 22s\n",
      "Bleu Score on Validation set = 22.97\n",
      "Epoch 16/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 124m 54s\n",
      "Bleu Score on Validation set = 22.72\n",
      "Epoch 17/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 23s\n",
      "Bleu Score on Validation set = 23.10\n",
      "Epoch 18/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 21s\n",
      "Bleu Score on Validation set = 23.17\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 19/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 20s\n",
      "Bleu Score on Validation set = 22.89\n",
      "Epoch 20/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 20s\n",
      "Bleu Score on Validation set = 23.19\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 21/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 18s\n",
      "Bleu Score on Validation set = 22.65\n",
      "Epoch 22/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 18s\n",
      "Bleu Score on Validation set = 22.64\n",
      "Epoch 23/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 19s\n",
      "Bleu Score on Validation set = 23.23\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 24/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 20s\n",
      "Bleu Score on Validation set = 23.32\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 25/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 21s\n",
      "Bleu Score on Validation set = 23.41\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 26/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 21s\n",
      "Bleu Score on Validation set = 23.54\n",
      "Saving Checkpoint...\n",
      "Saved!\n",
      "Epoch 27/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 143m 52s\n",
      "Bleu Score on Validation set = 23.17\n",
      "Epoch 28/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 24s\n",
      "Bleu Score on Validation set = 23.29\n",
      "Epoch 29/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 21s\n",
      "Bleu Score on Validation set = 23.22\n",
      "Epoch 30/30\n",
      "Translated Example Sentence: \n",
      " a boat carrying several men is pulled to shore by a large team of horses \n",
      "Time taken: 3m 26s\n",
      "Bleu Score on Validation set = 23.23\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "    model.eval() # Will turn off dropout\n",
    "    translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=60)\n",
    "    translated_sentence_final = ''\n",
    "    for i, word in enumerate(translated_sentence[:-1]):\n",
    "        if i != len(translated_sentence)-1:\n",
    "            translated_sentence_final+=word+' '\n",
    "        else:\n",
    "            translated_sentence_final+=word+'.'\n",
    "    print(f\"Translated Example Sentence: \\n {translated_sentence_final}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tic = time.time()\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        input_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        output = model(input_data, target)\n",
    "        # output shape: (trg_len, batch_size, output_dim)\n",
    "\n",
    "        output = output[1:].reshape(-1, output.shape[2]) # So that we can send it to a softmax in CrossEntropyLoss function\n",
    "        target = target[1:].reshape(-1) # Shape (trg_len * batch_size)\n",
    "\n",
    "        # We are doing this so that the loss of all the words predicted can be done at once\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('Training Loss', loss, global_step = step)\n",
    "        step+=1\n",
    "    print(f\"Time taken: {(time.time() - tic)//60:.0f}m {(time.time() - tic)%60:.0f}s\")\n",
    "    bleu_score_value = bleu(val_data, model, german, english, device)\n",
    "    print(f'Bleu Score on Validation set = {bleu_score_value*100:.2f}')\n",
    "    if max1 < bleu_score_value:\n",
    "        max1 = bleu_score_value\n",
    "        checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (encoder): EncoderLSTM(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (embedding): Embedding(7805, 300)\n",
       "    (lstm): LSTM(300, 1024, bidirectional=True)\n",
       "    (hidden_fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (cell_fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (embedding): Embedding(5964, 300)\n",
       "    (lstm): LSTM(2348, 1024)\n",
       "    (energy_alignment_model): Linear(in_features=3072, out_features=1, bias=True)\n",
       "    (softmax_energy): Softmax(dim=0)\n",
       "    (relu): ReLU()\n",
       "    (fc): Linear(in_features=1024, out_features=5964, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"models_state_dict/Seq2Seq_Attention_checkpoint.pth.tar\"), model, optimizer)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score on train data = 96.72\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bleu Score on train data = {bleu(train_data, model, german, english, device)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score on test data = 23.27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bleu Score on test data = {bleu(test_data, model, german, english, device)*100:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Es gibt so viele verschiedene Möglichkeiten für Eiscreme\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected translation**: *There is so much variety in the options for icecream available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Example Sentence: \n",
      " it that many things set very <unk> \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=60)\n",
    "    translated_sentence_final = ''\n",
    "    for i, word in enumerate(translated_sentence[:-1]):\n",
    "        if i != len(translated_sentence)-1:\n",
    "            translated_sentence_final+=word+' '\n",
    "        else:\n",
    "            translated_sentence_final+=word+'.'\n",
    "    print(f\"Translated Example Sentence: \\n {translated_sentence_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193f6b5c64d175a70f8bc370a8e28557b54eddf9787b8dde324aa4d68183bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
