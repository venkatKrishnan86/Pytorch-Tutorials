{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "where -\n",
    "- $d_k$: Embedding size\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "<p align=\"center\">\n",
    "<img src=\"../images/Multi-Head Self Attention.png\" style=\"width:450px;height:250px;\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_size, heads) -> None:\n",
    "        \"\"\"\n",
    "            embedding_size: Dimension of embedding\n",
    "            heads: Number of splits on the embedding\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "        self.head_dimension = embedding_size//heads \n",
    "        # Example: 256 embedding dimension and 8 heads = Each head will have a dimension of 32\n",
    "        \n",
    "        # In case we end up sending a non-divisible value, we will end up with the last head having a different dimension, which we don't want\n",
    "        assert (self.head_dimension*heads == embedding_size), \"Embedding size needs to be divisible by the heads\"\n",
    "\n",
    "        # Define the linear layers to apply on the input to obtain Q, K and V\n",
    "        self.W_Q = nn.Linear(self.head_dimension, self.head_dimension, bias = False) # W_Q\n",
    "        self.W_K = nn.Linear(self.head_dimension, self.head_dimension, bias = False) # W_K\n",
    "        self.W_V = nn.Linear(self.head_dimension, self.head_dimension, bias = False) # W_V\n",
    "        self.fc_out = nn.Linear(heads*self.head_dimension, embedding_size) # Or embedding_size --> embedding_size\n",
    "\n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        # query = (batch_size, query_len, embedding_dim)\n",
    "        # keys = (batch_size, key_len, embedding_dim)\n",
    "        # values = (batch_size, value_len, embedding_dim)\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # The length of Q, K and V will DEPEND on the target or source sentence length\n",
    "        # Since here we don't know where it will be used, either for encoder or decoder, we cannot fix the length to be of the source or the target\n",
    "        # So they will vary based on where it is used\n",
    "        # We will think of the all the len variables as the sentence length\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        query = query.reshape(batch_size, query_len, self.heads, self.head_dimension)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dimension)\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dimension)\n",
    "\n",
    "        query = self.W_Q(query)\n",
    "        keys = self.W_K(keys)\n",
    "        values = self.W_V(values)\n",
    "\n",
    "        # New shapes -\n",
    "        # query = (batch_size, query_len, heads, head_dim)\n",
    "        # keys = (batch_size, key_len, heads, head_dim)\n",
    "        # values = (batch_size, value_len, heads, head_dim)\n",
    "\n",
    "        # Next step: Multiply query with keys and name it energy\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", query, keys)\n",
    "        # n: Batch Size\n",
    "        # q: Query length\n",
    "        # k: Key length\n",
    "        # h: Heads\n",
    "        # d: Head dimension\n",
    "        # energy shape: (batch_size, heads, query_len, key_len)\n",
    "\n",
    "        if mask is not None: # Add mask\n",
    "            energy = energy.masked_fill(mask == 0 , value = float(\"-1e20\")) \n",
    "            # If mask == 0, then we want to shut that off\n",
    "            # Mask for the target will be a TRIANGULAR MATRIX\n",
    "            # The element when we will close it will be zero, so will replace it with -infinity\n",
    "            # This will result in the softmax becoming 0\n",
    "\n",
    "        # Attention(Q,K,V) = softmax(QK^T/sqrt(embedding_size))V\n",
    "        attention = torch.softmax(energy/(self.embedding_size**(0.5)), dim = 3) # Normalizing across the key_length (can be source or target sentence)\n",
    "        out = torch.einsum(\"nhql, nlhd -> nqhd\", attention, values)\n",
    "        # value_len and key_len will ALWAYS have the same length\n",
    "        # query_len can be different based on if it is from the source or target length\n",
    "        # attention shape: (batch_size, heads, query_len, key_len)\n",
    "        # values shape: (batch_size, value_len, heads, heads_dim)\n",
    "        # We want out shape to be: (batch_size, query_len, heads, head_dim)\n",
    "\n",
    "        out = out.reshape(batch_size, query_len, self.heads*self.head_dimension) \n",
    "        # Concatenate back to embedding dimension (Flattening last two dimensions)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.tensor([[0,2][3,4]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand how `masked_fill()` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(5,5))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6378,  1.1017, -1.1976, -0.3224, -2.5808],\n",
       "        [ 1.3692,  1.3641,  0.2511,  0.6175, -1.0132],\n",
       "        [-1.0865,  0.9378,  0.1768, -0.2069, -0.6674],\n",
       "        [ 0.4720,  0.0201, -0.3909, -0.0814, -0.7656],\n",
       "        [-1.0700,  1.9985,  1.7667,  0.6541,  1.3015]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = torch.randn(5,5)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.6378, -10.0000, -10.0000, -10.0000, -10.0000],\n",
       "        [  1.3692,   1.3641, -10.0000, -10.0000, -10.0000],\n",
       "        [ -1.0865,   0.9378,   0.1768, -10.0000, -10.0000],\n",
       "        [  0.4720,   0.0201,  -0.3909,  -0.0814, -10.0000],\n",
       "        [ -1.0700,   1.9985,   1.7667,   0.6541,   1.3015]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.masked_fill(mask==0, value = -10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder Block\n",
    "<p align=\"center\">\n",
    "<img src=\"../images/Transformer Encoder Block.png\" style=\"width:200px;height:300px;\">\n",
    "</p>\n",
    "\n",
    "This is the Encoder block which we shall implement in the `TransformerEncoderBlock` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, dropout = 0.1, forward_expansion = 4) -> None:\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.multiHeadAttention = MultiHeadAttention(embedding_size = embedding_size, heads = heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        # BatchNorm: Takes average across batch\n",
    "        # LayerNorm: Takes average across all examples\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_size, forward_expansion*embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embedding_size, embedding_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, keys, values, mask = None):\n",
    "        attention = self.multiHeadAttention(query, keys, values, mask)\n",
    "        x_mid = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x_mid)\n",
    "        out = self.dropout(self.norm2(forward + x_mid))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        source_vocab_size, # We will create the embeddings as well\n",
    "        embedding_size, # We will create the embeddings as well\n",
    "        num_layers, # Number of Encoder Layers\n",
    "        heads, \n",
    "        device, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        max_length \n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "            max_length: Related to the positional embedding. We need to send\n",
    "            in the max sentence length, for example, if we have most of the \n",
    "            sentences of length around 30 - 70 and a couple of sentences of\n",
    "            length 1000, we will set the max_length of 100, which will remove\n",
    "            the 1000 length sentences and keep the normal size of the data\n",
    "            (Generally 100 depending on the data)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.device = device\n",
    "\n",
    "        # Trainable Embeddings\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=source_vocab_size, embedding_dim=embedding_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embedding_size) # Trainable Positional Embeddings\n",
    "\n",
    "        self.layers =nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderBlock(\n",
    "                    embedding_size = embedding_size,\n",
    "                    heads = heads,\n",
    "                    dropout = dropout,\n",
    "                    forward_expansion = forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers) # Create num_layers objects of TransformerEncoderBlock\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        batch_size, sequence_length = x.shape # Input words\n",
    "        positions = torch.arange(0, sequence_length).expand(batch_size, sequence_length).to(self.device)\n",
    "\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder Block\n",
    "<p align=\"center\">\n",
    "<img src=\"../images/Transformer Decoder Block.png\" style=\"width:200px;height:400px;\">\n",
    "</p>\n",
    "\n",
    "This is the Decoder block which we shall implement in the `TransformerDecoderBlock` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size,\n",
    "        heads, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        device\n",
    "        ) -> None:\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embedding_size, heads)\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "        self.encoder_block = TransformerEncoderBlock(\n",
    "            embedding_size,\n",
    "            heads,\n",
    "            dropout,\n",
    "            forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\" \n",
    "            trg mask: Triangular mask\n",
    "            src mask: \n",
    "            Optional - Generally only used on the padded part of the sentence\n",
    "        \"\"\"\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.encoder_block(query, key, value, src_mask)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n",
    "<p align=\"center\">\n",
    "<img src=\"../images/Transformer Decoder.png\" style=\"width:150px;height:450px;\">\n",
    "</p>\n",
    "\n",
    "This is the Decoder which we shall implement in the `Decoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_vocab_size,\n",
    "        embedding_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length\n",
    "        ) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embedding_size)\n",
    "        self.positional_embedding = nn.Embedding(max_length, embedding_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderBlock(\n",
    "                    embedding_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                    device\n",
    "                )\n",
    "                for _ in range(num_layers) # Create num_layers objects of TransformerDecoderBlock\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Will get an input of shape: (batch_size, query_len, embedding_size)\n",
    "        self.fc_out = nn.Linear(embedding_size, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        batch_size, sequence_length = x.shape # Input words\n",
    "        positions = torch.arange(0, sequence_length).expand(batch_size, sequence_length).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.positional_embedding(x))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, value = enc_out, key = enc_out, src_mask = src_mask, trg_mask = trg_mask)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_vocab_size,\n",
    "        target_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embedding_size = 256,\n",
    "        num_layers = 6,\n",
    "        forward_expansion = 4,\n",
    "        heads = 8,\n",
    "        dropout = 0.1,\n",
    "        device = \"cpu\",\n",
    "        max_length = 100\n",
    "        ) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            source_vocab_size,\n",
    "            embedding_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            target_vocab_size,\n",
    "            embedding_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    # Make the functions to create the mask\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # src shape: (batch_size, src_len)\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2) # Adding 2 dimensions\n",
    "        # src_mask shape: (batch_size, 1, 1, src_len)\n",
    "        # If it is a src pad index, it will be set to 0, else 1\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # Create a triangular matrix\n",
    "        # We want a LOWER TRIANGULAR MATRIX with the lower part having 1s and rest 0s\n",
    "        # We will use torch.tril() for creating a lower triangular matrix\n",
    "        batch_size, trg_length = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_length, trg_length))).expand(batch_size, 1, trg_length, trg_length)\n",
    "        # We shall expand it to obtain a mask for each training example\n",
    "        return trg_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        src_mask = self.make_src_mask(source)\n",
    "        trg_mask = self.make_src_mask(target)\n",
    "        enc_src = self.encoder(x = source, mask = src_mask)\n",
    "        dec_out = self.decoder(x = target, enc_out = enc_src, src_mask = src_mask, trg_mask = trg_mask)\n",
    "        return dec_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand `torch.tril()` and `torch.triu()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only ones - \n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "Lower Triangular Matrix - \n",
      " tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print('Only ones - \\n',torch.ones((5,5)))\n",
    "print('\\nLower Triangular Matrix - \\n',torch.tril(torch.ones((5,5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another Example with random numbers- \n",
      " tensor([[-0.0524, -1.5357, -0.9137,  0.7272, -0.0664],\n",
      "        [-0.1379,  1.5173, -0.9358, -0.1186, -1.7448],\n",
      "        [-1.7164, -0.0762, -0.1048, -1.0726,  0.1657],\n",
      "        [-0.6807,  0.2704, -1.0409,  0.0774,  0.4895],\n",
      "        [-0.2081,  0.7440, -0.9311,  0.0226,  0.6382]])\n",
      "\n",
      "Lower Triangular version - \n",
      " tensor([[-0.0524,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1379,  1.5173,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.7164, -0.0762, -0.1048,  0.0000,  0.0000],\n",
      "        [-0.6807,  0.2704, -1.0409,  0.0774,  0.0000],\n",
      "        [-0.2081,  0.7440, -0.9311,  0.0226,  0.6382]])\n",
      "\n",
      "Upper Triangular version - \n",
      " tensor([[-0.0524, -1.5357, -0.9137,  0.7272, -0.0664],\n",
      "        [ 0.0000,  1.5173, -0.9358, -0.1186, -1.7448],\n",
      "        [ 0.0000,  0.0000, -0.1048, -1.0726,  0.1657],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0774,  0.4895],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6382]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((5,5))\n",
    "print('Another Example with random numbers- \\n', a)\n",
    "print('\\nLower Triangular version - \\n',torch.tril(a))\n",
    "print('\\nUpper Triangular version - \\n',torch.triu(a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193f6b5c64d175a70f8bc370a8e28557b54eddf9787b8dde324aa4d68183bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
